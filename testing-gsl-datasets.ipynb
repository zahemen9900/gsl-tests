{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQU01pFRn01u",
    "outputId": "395eb6a6-e28d-4c63-dbd3-5206d22b7658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1101M  100 1101M    0     0  5772k      0  0:03:15  0:03:15 --:--:-- 15.1M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#!/bin/bash\n",
    "!curl -L -o openpose_data.zip\\\n",
    "  https://zenodo.org/records/4533753/files/GSL_openpose_data.zip?download=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Sn2vguPOoTA1",
    "outputId": "503edd28-85d6-49d7-e765-642f7bbe1001"
   },
   "outputs": [],
   "source": [
    "!unzip openpose_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2ePQJ1CXq9O",
    "outputId": "be0ae156-8204-434e-f376-12ed2d3c10c0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "!ls -R /content/GSL_openpose_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9VkMig0LtnO",
    "outputId": "fc9e36ce-199a-4f4d-abcb-0bb41ae0a2e6"
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "!curl -L -o signtalk-ghana.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/responsibleailab/signtalk-ghana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "zMdHr9LlUuV4",
    "outputId": "57435b63-1480-46fb-b741-c31e341edac6"
   },
   "outputs": [],
   "source": [
    "!unzip signtalk-ghana.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOJUKfGfJo6E",
    "outputId": "d7cc3698-9f2e-4962-b989-03c0b37b807b"
   },
   "outputs": [],
   "source": [
    "!pip install pandas openpyxl tqdm\n",
    "import os, shutil, random, re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "print(\"âœ… Dependencies installed and ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I14z5V0fLXnY",
    "outputId": "44fa11f3-d1c2-4c40-8401-c3aed10b8ac6"
   },
   "outputs": [],
   "source": [
    "# Adjust these paths to match your Drive structure\n",
    "DATASET_DIR = \"/content/SignTalk-GH\"  # Change if different\n",
    "VIDEO_DIR = os.path.join(DATASET_DIR, \"Videos\")\n",
    "META_PATH = os.path.join(DATASET_DIR, \"Metadata.xlsx\")\n",
    "\n",
    "# Verify paths exist\n",
    "assert os.path.exists(DATASET_DIR), f\"âŒ Dataset dir not found: {DATASET_DIR}\"\n",
    "assert os.path.exists(META_PATH), f\"âŒ Metadata.xlsx not found: {META_PATH}\"\n",
    "assert os.path.isdir(VIDEO_DIR), f\"âŒ Videos dir not found: {VIDEO_DIR}\"\n",
    "\n",
    "print(f\"âœ… Dataset directory: {DATASET_DIR}\")\n",
    "print(f\"âœ… Video directory: {VIDEO_DIR}\")\n",
    "print(f\"âœ… Metadata file: {META_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "dsk-4PGFLeIh",
    "outputId": "fc8c041a-77cc-4da0-ec86-efed6e27ef1a"
   },
   "outputs": [],
   "source": [
    "# Load the metadata Excel file\n",
    "df_sentences = pd.read_excel(META_PATH, sheet_name=\"Sheet1\")\n",
    "\n",
    "print(f\"ðŸ“Š Loaded {len(df_sentences)} unique sentences\")\n",
    "print(f\"ðŸ“‹ Columns: {df_sentences.columns.tolist()}\")\n",
    "print(\"\\nðŸ“ Sample rows:\")\n",
    "df_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate GSL dictionary videos into SignTalk schema\n",
    "from collections import defaultdict\n",
    "\n",
    "GSL_DIR = \"/content/GSL_openpose_data\"\n",
    "VARIANT_CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "\n",
    "def _title_keep_hyphen(word: str) -> str:\n",
    "    parts = [seg.capitalize() for seg in word.split('-') if seg]\n",
    "    return \"-\".join(parts)\n",
    "\n",
    "\n",
    "def normalize_phrase_token(token: str) -> str:\n",
    "    token = token.strip()\n",
    "    token = token.replace(\"_AND_\", \" AND \")\n",
    "    token = token.replace(\"_\", \" \")\n",
    "    token = re.sub(r\"\\s+\", \" \", token).strip()\n",
    "    words = [_title_keep_hyphen(w.lower()) if w else \"\" for w in token.split(\" \")]\n",
    "    return \" \".join(words).strip()\n",
    "\n",
    "\n",
    "def split_concept_tokens(raw: str) -> list:\n",
    "    pieces = re.split(r\"_OR_\", raw)\n",
    "    cleaned = []\n",
    "    for piece in pieces:\n",
    "        piece = piece.strip()\n",
    "        if not piece:\n",
    "            continue\n",
    "        cleaned.append(normalize_phrase_token(piece))\n",
    "    return cleaned if cleaned else [normalize_phrase_token(raw)]\n",
    "\n",
    "\n",
    "folder_pattern = re.compile(r\"^(.+?)(?:_(\\d+))?$\")\n",
    "\n",
    "if not os.path.exists(GSL_DIR):\n",
    "    print(f\"âš ï¸ GSL directory not found at {GSL_DIR}. Skipping integration.\")\n",
    "else:\n",
    "    gsl_entries = defaultdict(list)\n",
    "    all_folders = [d for d in os.listdir(GSL_DIR) if os.path.isdir(os.path.join(GSL_DIR, d))]\n",
    "\n",
    "    for folder_name in tqdm(sorted(all_folders), desc=\"Parsing GSL folders\"):\n",
    "        match = folder_pattern.match(folder_name)\n",
    "        if not match:\n",
    "            continue\n",
    "        base_concept, variant_idx = match.group(1), match.group(2)\n",
    "        folder_path = os.path.join(GSL_DIR, folder_name)\n",
    "        mp4_files = [f for f in os.listdir(folder_path) if f.lower().endswith(\".mp4\")]\n",
    "        if not mp4_files:\n",
    "            continue\n",
    "        mp4_files.sort()\n",
    "        gsl_entries[base_concept].append({\n",
    "            \"variant_idx\": int(variant_idx) if variant_idx is not None else 0,\n",
    "            \"path\": os.path.join(folder_path, mp4_files[0])\n",
    "        })\n",
    "\n",
    "    print(f\"Found {len(gsl_entries)} base concepts with videos.\")\n",
    "\n",
    "    current_id = int(df_sentences[\"Sentence ID\"].max()) + 1\n",
    "    new_rows = []\n",
    "    copied = 0\n",
    "    failed = []\n",
    "    skipped_existing = 0\n",
    "    existing_texts = set(df_sentences[\"Sentence Text\"].str.lower())\n",
    "\n",
    "    for base_concept, items in tqdm(gsl_entries.items(), desc=\"Integrating GSL concepts\"):\n",
    "        phrase_texts = split_concept_tokens(base_concept)\n",
    "        variant_items = sorted(items, key=lambda x: x[\"variant_idx\"])\n",
    "        variant_paths = [v[\"path\"] for v in variant_items]\n",
    "\n",
    "        if not variant_paths:\n",
    "            continue\n",
    "\n",
    "        for phrase in phrase_texts:\n",
    "            key = phrase.lower()\n",
    "            if key in existing_texts:\n",
    "                skipped_existing += 1\n",
    "                continue\n",
    "\n",
    "            sid = current_id\n",
    "            current_id += 1\n",
    "            new_rows.append({\n",
    "                \"Sentence ID\": sid,\n",
    "                \"Sentence Text\": phrase,\n",
    "                \"Category\": \"GSL Dictionary\",\n",
    "            })\n",
    "            existing_texts.add(key)\n",
    "\n",
    "            for idx, src_path in enumerate(variant_paths):\n",
    "                if idx >= len(VARIANT_CHARS):\n",
    "                    failed.append({\"sentence_id\": sid, \"reason\": \"variant_limit\", \"src\": src_path})\n",
    "                    continue\n",
    "                variant_token = VARIANT_CHARS[idx]\n",
    "                new_name = f\"{sid}{variant_token}.mp4\"\n",
    "                dst_path = os.path.join(VIDEO_DIR, new_name)\n",
    "                try:\n",
    "                    shutil.copy2(src_path, dst_path)\n",
    "                    copied += 1\n",
    "                except Exception as exc:\n",
    "                    failed.append({\"sentence_id\": sid, \"src\": src_path, \"error\": str(exc)})\n",
    "\n",
    "    if new_rows:\n",
    "        df_new = pd.DataFrame(new_rows)\n",
    "        df_sentences = pd.concat([df_sentences, df_new], ignore_index=True)\n",
    "        df_sentences[\"Sentence ID\"] = df_sentences[\"Sentence ID\"].astype(int)\n",
    "        print(f\"âœ… Added {len(df_new)} new GSL sentences/phrases.\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ No new GSL rows added.\")\n",
    "\n",
    "    print(f\"ðŸ“¦ Copied videos: {copied}\")\n",
    "    if skipped_existing:\n",
    "        print(f\"â„¹ï¸ Skipped existing sentences: {skipped_existing}\")\n",
    "    if failed:\n",
    "        print(f\"âš ï¸ Failed copies: {len(failed)}\")\n",
    "        failed_path = \"/content/gsl_failed_copies.csv\"\n",
    "        pd.DataFrame(failed).to_csv(failed_path, index=False)\n",
    "        print(f\"   Saved details to {failed_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "I6OqcO0HLhR3",
    "outputId": "19a69031-b1d6-4c3c-bd15-922f7a0b9668"
   },
   "outputs": [],
   "source": [
    "# Get all video files\n",
    "video_files = [f for f in os.listdir(VIDEO_DIR) if f.lower().endswith('.mp4')]\n",
    "print(f\"ðŸ“¹ Found {len(video_files)} video files\")\n",
    "\n",
    "# Create video-to-sentence mapping\n",
    "video_mapping = []\n",
    "unmapped = []\n",
    "\n",
    "# Pattern to extract sentence ID from filename (e.g., \"1A.mp4\" â†’ 1)\n",
    "pattern = re.compile(r'^(\\d+)[A-Za-z]?\\.mp4$', re.IGNORECASE)\n",
    "\n",
    "def extract_variant_token(video_name: str, sentence_id: int) -> str:\n",
    "    \"\"\"Approximate signer/variant token from filename suffix.\"\"\"\n",
    "    stem = Path(video_name).stem\n",
    "    suffix = stem.replace(str(sentence_id), \"\", 1).strip().upper()\n",
    "    return suffix if suffix else \"BASE\"\n",
    "\n",
    "for video_file in tqdm(video_files, desc=\"Mapping videos to sentences\"):\n",
    "    match = pattern.match(video_file)\n",
    "\n",
    "    if not match:\n",
    "        unmapped.append(video_file)\n",
    "        continue\n",
    "\n",
    "    # Extract sentence ID\n",
    "    sentence_id = int(match.group(1))\n",
    "\n",
    "    # Look up sentence info from metadata\n",
    "    sentence_row = df_sentences[df_sentences['Sentence ID'] == sentence_id]\n",
    "\n",
    "    if sentence_row.empty:\n",
    "        unmapped.append(video_file)\n",
    "        continue\n",
    "\n",
    "    # Create mapping entry\n",
    "    video_mapping.append({\n",
    "        'video_file': video_file,\n",
    "        'sentence_id': sentence_id,\n",
    "        'sentence': sentence_row.iloc[0]['Sentence Text'],\n",
    "        'category': sentence_row.iloc[0]['Category'],  # 'Category' column is the category\n",
    "        'variant': extract_variant_token(video_file, sentence_id)\n",
    "    })\n",
    "\n",
    "# Create DataFrame from mapping\n",
    "df_videos = pd.DataFrame(video_mapping)\n",
    "\n",
    "print(f\"\\nâœ… Successfully mapped {len(df_videos)} videos\")\n",
    "print(f\"ðŸ“Š Covering {df_videos['sentence_id'].nunique()} unique sentences\")\n",
    "print(f\"ðŸ“ˆ Average videos per sentence: {len(df_videos) / df_videos['sentence_id'].nunique():.2f}\")\n",
    "print(f\"ðŸŽ¬ Approx variants per sentence: {df_videos.groupby('sentence_id')['variant'].nunique().mean():.2f}\")\n",
    "print(f\"âš ï¸ Unmapped videos: {len(unmapped)}\")\n",
    "\n",
    "if unmapped:\n",
    "    print(f\"\\nâš ï¸ Sample unmapped files: {unmapped[:5]}\")\n",
    "    pd.DataFrame({'unmapped_file': unmapped}).to_csv('/content/unmapped_videos.csv', index=False)\n",
    "\n",
    "df_videos.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bGeVL1_BU7_a",
    "outputId": "0f98f42b-8daa-4223-cf95-d69c8306b987"
   },
   "outputs": [],
   "source": [
    "DOUBLE_SAMPLE = False  # Flip to True for ~2400 videos\n",
    "TARGET_SAMPLE_COUNT = 2500 if DOUBLE_SAMPLE else 1500\n",
    "MIN_VARIANTS_PER_SENTENCE = 2 if DOUBLE_SAMPLE else 1\n",
    "MAX_VARIANTS_PER_SENTENCE = 4  # Avoid exploding duplicates\n",
    "\n",
    "# Get category distribution\n",
    "category_counts = df_videos['category'].value_counts()\n",
    "print(\"ðŸ“Š Category distribution in full dataset:\")\n",
    "print(category_counts)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Target clips: {TARGET_SAMPLE_COUNT}\")\n",
    "print(f\"ðŸ“‹ Categories: {len(category_counts)}\")\n",
    "print(f\"ï¿½ Min variants per sentence: {MIN_VARIANTS_PER_SENTENCE}\")\n",
    "\n",
    "# Initial stratified sampling identical to baseline approach\n",
    "num_categories = len(category_counts)\n",
    "samples_per_category = TARGET_SAMPLE_COUNT // max(1, num_categories)\n",
    "sampled_videos = []\n",
    "\n",
    "for category in category_counts.index:\n",
    "    category_videos = df_videos[df_videos['category'] == category]\n",
    "    n_samples = min(len(category_videos), samples_per_category)\n",
    "    if n_samples > 0:\n",
    "        sampled = category_videos.sample(n=n_samples, random_state=RANDOM_SEED)\n",
    "        sampled_videos.append(sampled)\n",
    "\n",
    "df_sampled = pd.concat(sampled_videos, ignore_index=True)\n",
    "\n",
    "# Top up if we are still short of the target\n",
    "if len(df_sampled) < TARGET_SAMPLE_COUNT:\n",
    "    remaining = df_videos[~df_videos['video_file'].isin(df_sampled['video_file'])]\n",
    "    additional_needed = TARGET_SAMPLE_COUNT - len(df_sampled)\n",
    "    if len(remaining) > 0:\n",
    "        additional = remaining.sample(\n",
    "            n=min(len(remaining), additional_needed),\n",
    "            random_state=RANDOM_SEED\n",
    "        )\n",
    "        df_sampled = pd.concat([df_sampled, additional], ignore_index=True)\n",
    "\n",
    "# Ensure sentences have the desired number of variants when available\n",
    "if MIN_VARIANTS_PER_SENTENCE > 1:\n",
    "    per_sentence_counts = df_sampled.groupby('sentence_id')['video_file'].count()\n",
    "    needs_boost = per_sentence_counts[per_sentence_counts < MIN_VARIANTS_PER_SENTENCE].index.tolist()\n",
    "    extras = []\n",
    "    for sid in needs_boost:\n",
    "        candidates = df_videos[\n",
    "            (df_videos['sentence_id'] == sid) & (~df_videos['video_file'].isin(df_sampled['video_file']))\n",
    "        ]\n",
    "        if candidates.empty:\n",
    "            continue\n",
    "        unique_variants = candidates.groupby('variant', group_keys=False).head(1)\n",
    "        take = min(\n",
    "            MIN_VARIANTS_PER_SENTENCE - per_sentence_counts.get(sid, 0),\n",
    "            len(unique_variants)\n",
    "        )\n",
    "        if take > 0:\n",
    "            extras.append(unique_variants.sample(n=take, random_state=RANDOM_SEED))\n",
    "    if extras:\n",
    "        extra_df = pd.concat(extras, ignore_index=True)\n",
    "        df_sampled = pd.concat([df_sampled, extra_df], ignore_index=True)\n",
    "\n",
    "# Respect per-sentence cap\n",
    "df_sampled = (\n",
    "    df_sampled\n",
    "    .sort_values('sentence_id')\n",
    "    .drop_duplicates('video_file')\n",
    "    .groupby('sentence_id', group_keys=False)\n",
    "    .head(MAX_VARIANTS_PER_SENTENCE)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# If we overshoot the target, drop whole sentences until we fit\n",
    "if len(df_sampled) > TARGET_SAMPLE_COUNT:\n",
    "    sentence_ids = df_sampled['sentence_id'].unique().tolist()\n",
    "    random.Random(RANDOM_SEED).shuffle(sentence_ids)\n",
    "    keep_ids = []\n",
    "    running = 0\n",
    "    for sid in sentence_ids:\n",
    "        block = df_sampled[df_sampled['sentence_id'] == sid]\n",
    "        block_len = len(block)\n",
    "        if running + block_len <= TARGET_SAMPLE_COUNT:\n",
    "            keep_ids.append(sid)\n",
    "            running += block_len\n",
    "    df_sampled = df_sampled[df_sampled['sentence_id'].isin(keep_ids)].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nâœ… Final sample size: {len(df_sampled)} videos\")\n",
    "print(f\"ðŸ“Š Covered sentences: {df_sampled['sentence_id'].nunique()}\")\n",
    "print(\"\\nðŸŽ­ Variants per sentence (min/mean/max):\",\n",
    "      df_sampled.groupby('sentence_id')['video_file'].count().agg(['min', 'mean', 'max']).to_dict())\n",
    "print(\"\\nðŸ“‹ Sampled category distribution:\")\n",
    "print(df_sampled['category'].value_counts())\n",
    "\n",
    "df_sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mg1-KZDaVDBf",
    "outputId": "8b8fd63c-5bce-4f98-9c6b-2fd8024dcaf5"
   },
   "outputs": [],
   "source": [
    "# Create output directory structure\n",
    "OUTPUT_DIR = \"/content/sample_dataset\"\n",
    "OUTPUT_VIDEO_DIR = os.path.join(OUTPUT_DIR, \"videos\")\n",
    "os.makedirs(OUTPUT_VIDEO_DIR, exist_ok=True)\n",
    "\n",
    "# Copy files\n",
    "copied_count = 0\n",
    "failed_copies = []\n",
    "\n",
    "for _, row in tqdm(df_sampled.iterrows(), total=len(df_sampled), desc=\"Copying videos\"):\n",
    "    src_path = os.path.join(VIDEO_DIR, row['video_file'])\n",
    "    dst_path = os.path.join(OUTPUT_VIDEO_DIR, row['video_file'])\n",
    "\n",
    "    try:\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "        copied_count += 1\n",
    "    except Exception as e:\n",
    "        failed_copies.append({\n",
    "            'video_file': row['video_file'],\n",
    "            'error': str(e)\n",
    "        })\n",
    "        print(f\"âŒ Failed to copy {row['video_file']}: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… Successfully copied: {copied_count}/{len(df_sampled)} videos\")\n",
    "print(f\"âŒ Failed copies: {len(failed_copies)}\")\n",
    "\n",
    "if failed_copies:\n",
    "    pd.DataFrame(failed_copies).to_csv(\n",
    "        os.path.join(OUTPUT_DIR, \"failed_copies.csv\"),\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPfuXOnpVJ4Y",
    "outputId": "24fee716-c2b7-4735-a5b1-973e9ac75fbe"
   },
   "outputs": [],
   "source": [
    "# Save sampled metadata\n",
    "metadata_path = os.path.join(OUTPUT_DIR, \"sampled_metadata.csv\")\n",
    "df_sampled.to_csv(metadata_path, index=False)\n",
    "print(f\"âœ… Saved metadata: {metadata_path}\")\n",
    "\n",
    "# Create summary statistics\n",
    "summary = {\n",
    "    'total_videos': len(df_sampled),\n",
    "    'unique_sentences': df_sampled['sentence_id'].nunique(),\n",
    "    'categories': df_sampled['category'].nunique(),\n",
    "    'videos_per_sentence_avg': len(df_sampled) / df_sampled['sentence_id'].nunique(),\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df.to_csv(os.path.join(OUTPUT_DIR, \"dataset_summary.csv\"), index=False)\n",
    "print(\"\\nðŸ“Š Dataset Summary:\")\n",
    "print(summary_df.T)\n",
    "\n",
    "# Zip the entire sampled dataset\n",
    "!cd /content && zip -r SignTalk-GH_Sampled.zip sample_dataset/\n",
    "\n",
    "print(\"\\nâœ… Zipped dataset: /content/SignTalk-GH_Sampled.zip\")\n",
    "print(\"ðŸ“¥ Download this file from Colab's file browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxfZSq3YVSox"
   },
   "outputs": [],
   "source": [
    "# Final verification\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ“Š SAMPLING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nðŸ“¹ Videos:\")\n",
    "print(f\"  â€¢ Total sampled: {len(df_sampled)}\")\n",
    "print(f\"  â€¢ Successfully copied: {copied_count}\")\n",
    "print(f\"  â€¢ Failed: {len(failed_copies)}\")\n",
    "\n",
    "print(f\"\\nðŸ“ Sentences:\")\n",
    "print(f\"  â€¢ Unique sentences covered: {df_sampled['sentence_id'].nunique()}\")\n",
    "print(f\"  â€¢ Original dataset sentences: {len(df_sentences)}\")\n",
    "print(f\"  â€¢ Coverage: {df_sampled['sentence_id'].nunique() / len(df_sentences) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Categories:\")\n",
    "print(df_sampled['category'].value_counts())\n",
    "\n",
    "print(f\"\\nâœ… Ready for local preprocessing!\")\n",
    "print(f\"ðŸ“¦ Download: /content/SignTalk-GH_Sampled.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U98_555OikH2"
   },
   "outputs": [],
   "source": [
    "!cp SignTalk-GH_Sampled.zip /content/drive/MyDrive"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
