
pytorch2.9.0+cu126:Ú
ì	
sequence
model.projector.norm.weight
model.projector.norm.bias
layer_normnode_layer_norm"LayerNormalization*

stash_type†*
epsilon¨≈'7†*
axisˇˇˇˇˇˇˇˇˇ†J≥
	namespace•: __main__.EncoderWrapper/model.projector: __main__.FrameProjector/model.projector.norm: torch.nn.modules.normalization.LayerNorm/layer_norm: aten.layer_norm.defaultJü
pkg.torch.onnx.class_hierarchy}['__main__.EncoderWrapper', '__main__.FrameProjector', 'torch.nn.modules.normalization.LayerNorm', 'aten.layer_norm.default']Jœ
pkg.torch.onnx.fx_node¥%layer_norm : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%x, [540], %p_model_projector_norm_weight, %p_model_projector_norm_bias), kwargs = {})J[
pkg.torch.onnx.name_scopes=['', 'model.projector', 'model.projector.norm', 'layer_norm']J⁄
pkg.torch.onnx.stack_traceªFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 13, in forward
    out = self.norm(x)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py", line 229, in forward
    return F.layer_norm(
Ö

layer_norm
val_2val_3node_MatMul_1"MatMulJ†
	namespaceí: __main__.EncoderWrapper/model.projector: __main__.FrameProjector/model.projector.fc1: torch.nn.modules.linear.Linear/linear: aten.linear.defaultJë
pkg.torch.onnx.class_hierarchyo['__main__.EncoderWrapper', '__main__.FrameProjector', 'torch.nn.modules.linear.Linear', 'aten.linear.default']J«
pkg.torch.onnx.fx_node¨%linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm, %p_model_projector_fc1_weight, %p_model_projector_fc1_bias), kwargs = {})JV
pkg.torch.onnx.name_scopes8['', 'model.projector', 'model.projector.fc1', 'linear']J¯
pkg.torch.onnx.stack_traceŸFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 14, in forward
    out = self.act(self.fc1(out))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
è
val_3
model.projector.fc1.biaslinearnode_linear"AddJ†
	namespaceí: __main__.EncoderWrapper/model.projector: __main__.FrameProjector/model.projector.fc1: torch.nn.modules.linear.Linear/linear: aten.linear.defaultJë
pkg.torch.onnx.class_hierarchyo['__main__.EncoderWrapper', '__main__.FrameProjector', 'torch.nn.modules.linear.Linear', 'aten.linear.default']J«
pkg.torch.onnx.fx_node¨%linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm, %p_model_projector_fc1_weight, %p_model_projector_fc1_bias), kwargs = {})JV
pkg.torch.onnx.name_scopes8['', 'model.projector', 'model.projector.fc1', 'linear']J¯
pkg.torch.onnx.stack_traceŸFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 14, in forward
    out = self.act(self.fc1(out))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
ª
linear
val_4val_5
node_Div_3"DivJû
	namespaceê: __main__.EncoderWrapper/model.projector: __main__.FrameProjector/model.projector.act: torch.nn.modules.activation.GELU/gelu: aten.gelu.defaultJë
pkg.torch.onnx.class_hierarchyo['__main__.EncoderWrapper', '__main__.FrameProjector', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÉ
pkg.torch.onnx.fx_nodei%gelu : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear,), kwargs = {})JT
pkg.torch.onnx.name_scopes6['', 'model.projector', 'model.projector.act', 'gelu']JÄ
pkg.torch.onnx.stack_trace·File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 14, in forward
    out = self.act(self.fc1(out))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
≥
val_5val_6
node_Erf_4"ErfJû
	namespaceê: __main__.EncoderWrapper/model.projector: __main__.FrameProjector/model.projector.act: torch.nn.modules.activation.GELU/gelu: aten.gelu.defaultJë
pkg.torch.onnx.class_hierarchyo['__main__.EncoderWrapper', '__main__.FrameProjector', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÉ
pkg.torch.onnx.fx_nodei%gelu : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear,), kwargs = {})JT
pkg.torch.onnx.name_scopes6['', 'model.projector', 'model.projector.act', 'gelu']JÄ
pkg.torch.onnx.stack_trace·File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 14, in forward
    out = self.act(self.fc1(out))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
∫
val_6
val_7val_8
node_Add_6"AddJû
	namespaceê: __main__.EncoderWrapper/model.projector: __main__.FrameProjector/model.projector.act: torch.nn.modules.activation.GELU/gelu: aten.gelu.defaultJë
pkg.torch.onnx.class_hierarchyo['__main__.EncoderWrapper', '__main__.FrameProjector', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÉ
pkg.torch.onnx.fx_nodei%gelu : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear,), kwargs = {})JT
pkg.torch.onnx.name_scopes6['', 'model.projector', 'model.projector.act', 'gelu']JÄ
pkg.torch.onnx.stack_trace·File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 14, in forward
    out = self.act(self.fc1(out))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ª
val_9
val_8val_10
node_Mul_8"MulJû
	namespaceê: __main__.EncoderWrapper/model.projector: __main__.FrameProjector/model.projector.act: torch.nn.modules.activation.GELU/gelu: aten.gelu.defaultJë
pkg.torch.onnx.class_hierarchyo['__main__.EncoderWrapper', '__main__.FrameProjector', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÉ
pkg.torch.onnx.fx_nodei%gelu : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear,), kwargs = {})JT
pkg.torch.onnx.name_scopes6['', 'model.projector', 'model.projector.act', 'gelu']JÄ
pkg.torch.onnx.stack_trace·File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 14, in forward
    out = self.act(self.fc1(out))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
∫
linear
val_10gelu	node_gelu"MulJû
	namespaceê: __main__.EncoderWrapper/model.projector: __main__.FrameProjector/model.projector.act: torch.nn.modules.activation.GELU/gelu: aten.gelu.defaultJë
pkg.torch.onnx.class_hierarchyo['__main__.EncoderWrapper', '__main__.FrameProjector', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÉ
pkg.torch.onnx.fx_nodei%gelu : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear,), kwargs = {})JT
pkg.torch.onnx.name_scopes6['', 'model.projector', 'model.projector.act', 'gelu']JÄ
pkg.torch.onnx.stack_trace·File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 14, in forward
    out = self.act(self.fc1(out))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
É
gelu
val_11val_12node_MatMul_10"MatMulJ¢
	namespaceî: __main__.EncoderWrapper/model.projector: __main__.FrameProjector/model.projector.fc2: torch.nn.modules.linear.Linear/linear_1: aten.linear.defaultJë
pkg.torch.onnx.class_hierarchyo['__main__.EncoderWrapper', '__main__.FrameProjector', 'torch.nn.modules.linear.Linear', 'aten.linear.default']Jƒ
pkg.torch.onnx.fx_node©%linear_1 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%clone, %p_model_projector_fc2_weight, %p_model_projector_fc2_bias), kwargs = {})JX
pkg.torch.onnx.name_scopes:['', 'model.projector', 'model.projector.fc2', 'linear_1']J¯
pkg.torch.onnx.stack_traceŸFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 16, in forward
    out = self.act(self.fc2(out))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
ï
val_12
model.projector.fc2.biaslinear_1node_linear_1"AddJ¢
	namespaceî: __main__.EncoderWrapper/model.projector: __main__.FrameProjector/model.projector.fc2: torch.nn.modules.linear.Linear/linear_1: aten.linear.defaultJë
pkg.torch.onnx.class_hierarchyo['__main__.EncoderWrapper', '__main__.FrameProjector', 'torch.nn.modules.linear.Linear', 'aten.linear.default']Jƒ
pkg.torch.onnx.fx_node©%linear_1 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%clone, %p_model_projector_fc2_weight, %p_model_projector_fc2_bias), kwargs = {})JX
pkg.torch.onnx.name_scopes:['', 'model.projector', 'model.projector.fc2', 'linear_1']J¯
pkg.torch.onnx.stack_traceŸFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 16, in forward
    out = self.act(self.fc2(out))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
«
linear_1
val_4val_14node_Div_12"DivJ†
	namespaceí: __main__.EncoderWrapper/model.projector: __main__.FrameProjector/model.projector.act: torch.nn.modules.activation.GELU/gelu_1: aten.gelu.defaultJë
pkg.torch.onnx.class_hierarchyo['__main__.EncoderWrapper', '__main__.FrameProjector', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_1 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_1,), kwargs = {})JV
pkg.torch.onnx.name_scopes8['', 'model.projector', 'model.projector.act', 'gelu_1']JÄ
pkg.torch.onnx.stack_trace·File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 16, in forward
    out = self.act(self.fc2(out))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
æ
val_14val_15node_Erf_13"ErfJ†
	namespaceí: __main__.EncoderWrapper/model.projector: __main__.FrameProjector/model.projector.act: torch.nn.modules.activation.GELU/gelu_1: aten.gelu.defaultJë
pkg.torch.onnx.class_hierarchyo['__main__.EncoderWrapper', '__main__.FrameProjector', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_1 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_1,), kwargs = {})JV
pkg.torch.onnx.name_scopes8['', 'model.projector', 'model.projector.act', 'gelu_1']JÄ
pkg.torch.onnx.stack_trace·File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 16, in forward
    out = self.act(self.fc2(out))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
≈
val_15
val_7val_17node_Add_15"AddJ†
	namespaceí: __main__.EncoderWrapper/model.projector: __main__.FrameProjector/model.projector.act: torch.nn.modules.activation.GELU/gelu_1: aten.gelu.defaultJë
pkg.torch.onnx.class_hierarchyo['__main__.EncoderWrapper', '__main__.FrameProjector', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_1 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_1,), kwargs = {})JV
pkg.torch.onnx.name_scopes8['', 'model.projector', 'model.projector.act', 'gelu_1']JÄ
pkg.torch.onnx.stack_trace·File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 16, in forward
    out = self.act(self.fc2(out))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
≈
val_9
val_17val_19node_Mul_17"MulJ†
	namespaceí: __main__.EncoderWrapper/model.projector: __main__.FrameProjector/model.projector.act: torch.nn.modules.activation.GELU/gelu_1: aten.gelu.defaultJë
pkg.torch.onnx.class_hierarchyo['__main__.EncoderWrapper', '__main__.FrameProjector', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_1 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_1,), kwargs = {})JV
pkg.torch.onnx.name_scopes8['', 'model.projector', 'model.projector.act', 'gelu_1']JÄ
pkg.torch.onnx.stack_trace·File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 16, in forward
    out = self.act(self.fc2(out))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
»
linear_1
val_19gelu_1node_gelu_1"MulJ†
	namespaceí: __main__.EncoderWrapper/model.projector: __main__.FrameProjector/model.projector.act: torch.nn.modules.activation.GELU/gelu_1: aten.gelu.defaultJë
pkg.torch.onnx.class_hierarchyo['__main__.EncoderWrapper', '__main__.FrameProjector', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_1 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_1,), kwargs = {})JV
pkg.torch.onnx.name_scopes8['', 'model.projector', 'model.projector.act', 'gelu_1']JÄ
pkg.torch.onnx.stack_trace·File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 16, in forward
    out = self.act(self.fc2(out))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
©
gelu_1
val_20val_21node_MatMul_19"MatMulJ±
	namespace£: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.input_proj: torch.nn.modules.linear.Linear/linear_2: aten.linear.defaultJù
pkg.torch.onnx.class_hierarchy{['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.linear.Linear', 'aten.linear.default']Jœ
pkg.torch.onnx.fx_node¥%linear_2 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%gelu_1, %p_model_encoder_input_proj_weight, %p_model_encoder_input_proj_bias), kwargs = {})J[
pkg.torch.onnx.name_scopes=['', 'model.encoder', 'model.encoder.input_proj', 'linear_2']JÛ
pkg.torch.onnx.stack_trace‘File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 59, in forward
    seq = self.input_proj(x)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
æ
val_21
model.encoder.input_proj.biaslinear_2node_linear_2"AddJ±
	namespace£: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.input_proj: torch.nn.modules.linear.Linear/linear_2: aten.linear.defaultJù
pkg.torch.onnx.class_hierarchy{['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.linear.Linear', 'aten.linear.default']Jœ
pkg.torch.onnx.fx_node¥%linear_2 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%gelu_1, %p_model_encoder_input_proj_weight, %p_model_encoder_input_proj_bias), kwargs = {})J[
pkg.torch.onnx.name_scopes=['', 'model.encoder', 'model.encoder.input_proj', 'linear_2']JÛ
pkg.torch.onnx.stack_trace‘File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 59, in forward
    seq = self.input_proj(x)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
«
linear_2
model.encoder.pos_encoder.peaddnode_add"AddJÆ
	namespace†: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.pos_encoder: __main__.TemporalPositionalEncoding/add: aten.add.TensorJû
pkg.torch.onnx.class_hierarchy|['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', '__main__.TemporalPositionalEncoding', 'aten.add.Tensor']Jâ
pkg.torch.onnx.fx_nodeo%add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_2, %alias), kwargs = {})JW
pkg.torch.onnx.name_scopes9['', 'model.encoder', 'model.encoder.pos_encoder', 'add']J—
pkg.torch.onnx.stack_trace≤File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 60, in forward
    seq = self.pos_encoder(seq)
  File "/tmp/ipython-input-3079789015.py", line 36, in forward
    return self.dropout(x + self.pe[:, :length])
™
add
+model.encoder.encoder.layers.0.norm1.weight
)model.encoder.encoder.layers.0.norm1.biaslayer_norm_1node_layer_norm_1"LayerNormalization*

stash_type†*
epsilon¨≈'7†*
axisˇˇˇˇˇˇˇˇˇ†JÎ
	namespace›: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.norm1: torch.nn.modules.normalization.LayerNorm/layer_norm_1: aten.layer_norm.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.normalization.LayerNorm', 'aten.layer_norm.default']J˜
pkg.torch.onnx.fx_node‹%layer_norm_1 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%clone_1, [256], %p_model_encoder_encoder_layers_0_norm1_weight, %p_model_encoder_encoder_layers_0_norm1_bias), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.norm1', 'layer_norm_1']J≠
pkg.torch.onnx.stack_traceéFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 929, in forward
    self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py", line 229, in forward
    return F.layer_norm(
¨
layer_norm_1	transposenode_transpose"	Transpose*
perm@@ @†JÌ
	namespaceﬂ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jî
pkg.torch.onnx.fx_nodez%transpose : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%layer_norm_1, 1, 0), kwargs = {})J®
pkg.torch.onnx.name_scopesâ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'transpose']Jó
pkg.torch.onnx.stack_trace¯File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1454, in forward
    query = key = value = query.transpose(1, 0)
ü
	transpose
val_24val_25node_MatMul_21"MatMulJÌ
	namespaceﬂ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/linear_3: aten.linear.defaultJô
pkg.torch.onnx.class_hierarchyˆ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.linear.default']JÇ
pkg.torch.onnx.fx_nodeÁ%linear_3 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose, %p_model_encoder_encoder_layers_0_self_attn_in_proj_weight, %p_model_encoder_encoder_layers_0_self_attn_in_proj_bias), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'linear_3']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
…
val_25
5model.encoder.encoder.layers.0.self_attn.in_proj_biaslinear_3node_linear_3"AddJÌ
	namespaceﬂ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/linear_3: aten.linear.defaultJô
pkg.torch.onnx.class_hierarchyˆ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.linear.default']JÇ
pkg.torch.onnx.fx_nodeÁ%linear_3 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose, %p_model_encoder_encoder_layers_0_self_attn_in_proj_weight, %p_model_encoder_encoder_layers_0_self_attn_in_proj_bias), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'linear_3']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
±
linear_3
val_31view	node_view"Reshape*
	allowzero†JÁ
	namespaceŸ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jï
pkg.torch.onnx.fx_node{%view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_3, [64, 1, 3, 256]), kwargs = {})J£
pkg.torch.onnx.name_scopesÑ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'view']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
≥
view
val_32	unsqueezenode_unsqueeze"	UnsqueezeJÒ
	namespace„: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/unsqueeze: aten.unsqueeze.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.unsqueeze.default']Jç
pkg.torch.onnx.fx_nodes%unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view, 0), kwargs = {})J®
pkg.torch.onnx.name_scopesâ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'unsqueeze']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ã
	unsqueezetranspose_1node_transpose_1"	Transpose*
perm@@@@ @†JÔ
	namespace·: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_1: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jî
pkg.torch.onnx.fx_nodez%transpose_1 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%unsqueeze, 0, -2), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'transpose_1']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
§
transpose_1
val_33squeezenode_squeeze"SqueezeJÈ
	namespace€: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/squeeze: aten.squeeze.dimJñ
pkg.torch.onnx.class_hierarchyÛ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.squeeze.dim']Jç
pkg.torch.onnx.fx_nodes%squeeze : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%transpose_1, -2), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'squeeze']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¢
squeeze
val_34selectnode_select"Gather*
axis †JÁ
	namespaceŸ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/select: aten.select.intJï
pkg.torch.onnx.class_hierarchyÚ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.select.int']Jâ
pkg.torch.onnx.fx_nodeo%select : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%clone_2, 0, 0), kwargs = {})J•
pkg.torch.onnx.name_scopesÜ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'select']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¨
squeeze
val_35select_1node_select_1"Gather*
axis †JÈ
	namespace€: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/select_1: aten.select.intJï
pkg.torch.onnx.class_hierarchyÚ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.select.int']Jã
pkg.torch.onnx.fx_nodeq%select_1 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%clone_2, 0, 1), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'select_1']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¨
squeeze
val_36select_2node_select_2"Gather*
axis †JÈ
	namespace€: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/select_2: aten.select.intJï
pkg.torch.onnx.class_hierarchyÚ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.select.int']Jã
pkg.torch.onnx.fx_nodeq%select_2 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%clone_2, 0, 2), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'select_2']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
≥
select
val_41view_1node_view_1"Reshape*
	allowzero†JÈ
	namespace€: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_1: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jë
pkg.torch.onnx.fx_nodew%view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select, [64, 4, 64]), kwargs = {})J•
pkg.torch.onnx.name_scopesÜ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'view_1']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¡
view_1transpose_2node_transpose_2"	Transpose*
perm@@ @†JÔ
	namespace·: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_2: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jê
pkg.torch.onnx.fx_nodev%transpose_2 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_1, 0, 1), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'transpose_2']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
∑
select_1
val_41view_2node_view_2"Reshape*
	allowzero†JÈ
	namespace€: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_2: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jì
pkg.torch.onnx.fx_nodey%view_2 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select_1, [64, 4, 64]), kwargs = {})J•
pkg.torch.onnx.name_scopesÜ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'view_2']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¡
view_2transpose_3node_transpose_3"	Transpose*
perm@@ @†JÔ
	namespace·: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_3: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jê
pkg.torch.onnx.fx_nodev%transpose_3 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_2, 0, 1), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'transpose_3']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
∑
select_2
val_41view_3node_view_3"Reshape*
	allowzero†JÈ
	namespace€: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_3: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jì
pkg.torch.onnx.fx_nodey%view_3 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select_2, [64, 4, 64]), kwargs = {})J•
pkg.torch.onnx.name_scopesÜ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'view_3']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¡
view_3transpose_4node_transpose_4"	Transpose*
perm@@ @†JÔ
	namespace·: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_4: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jê
pkg.torch.onnx.fx_nodev%transpose_4 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_3, 0, 1), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'transpose_4']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¿
transpose_2
val_57view_4node_view_4"Reshape*
	allowzero†JÈ
	namespace€: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_4: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jô
pkg.torch.onnx.fx_node%view_4 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_2, [1, 4, 64, 64]), kwargs = {})J•
pkg.torch.onnx.name_scopesÜ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'view_4']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¿
transpose_4
val_57view_6node_view_6"Reshape*
	allowzero†JÈ
	namespace€: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_6: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jô
pkg.torch.onnx.fx_node%view_6 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_4, [1, 4, 64, 64]), kwargs = {})J•
pkg.torch.onnx.name_scopesÜ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'view_6']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
«
transpose_3val_89node_Transpose_85"	Transpose*
perm@ @@†Jó
	namespaceâ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J≈
pkg.torch.onnx.fx_node™%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_4, %view_5, %view_6), kwargs = {})Jª
pkg.torch.onnx.name_scopesú['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'scaled_dot_product_attention']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
«
val_89
val_57val_91node_Reshape_87"Reshape*
	allowzero †Jó
	namespaceâ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J≈
pkg.torch.onnx.fx_node™%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_4, %view_5, %view_6), kwargs = {})Jª
pkg.torch.onnx.name_scopesú['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'scaled_dot_product_attention']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
≠
view_4
val_92val_93node_Mul_89"MulJó
	namespaceâ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J≈
pkg.torch.onnx.fx_node™%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_4, %view_5, %view_6), kwargs = {})Jª
pkg.torch.onnx.name_scopesú['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'scaled_dot_product_attention']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
≠
val_91
val_92val_96node_Mul_92"MulJó
	namespaceâ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J≈
pkg.torch.onnx.fx_node™%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_4, %view_5, %view_6), kwargs = {})Jª
pkg.torch.onnx.name_scopesú['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'scaled_dot_product_attention']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
≥
val_93
val_96val_97node_MatMul_93"MatMulJó
	namespaceâ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J≈
pkg.torch.onnx.fx_node™%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_4, %view_5, %view_6), kwargs = {})Jª
pkg.torch.onnx.name_scopesú['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'scaled_dot_product_attention']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
√
val_97val_98node_Softmax_94"Softmax*
axisˇˇˇˇˇˇˇˇˇ†Jó
	namespaceâ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J≈
pkg.torch.onnx.fx_node™%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_4, %view_5, %view_6), kwargs = {})Jª
pkg.torch.onnx.name_scopesú['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'scaled_dot_product_attention']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
‹
val_98
view_6scaled_dot_product_attention!node_scaled_dot_product_attention"MatMulJó
	namespaceâ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J≈
pkg.torch.onnx.fx_node™%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_4, %view_5, %view_6), kwargs = {})Jª
pkg.torch.onnx.name_scopesú['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'scaled_dot_product_attention']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Í
scaled_dot_product_attentionpermutenode_permute"	Transpose*
perm@@ @@†JÌ
	namespaceﬂ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/permute: aten.permute.defaultJö
pkg.torch.onnx.class_hierarchy˜['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.permute.default']J≠
pkg.torch.onnx.fx_nodeí%permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%scaled_dot_product_attention, [2, 0, 1, 3]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'permute']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¥
permute
val_102view_7node_view_7"Reshape*
	allowzero†JÈ
	namespace€: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_7: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jê
pkg.torch.onnx.fx_nodev%view_7 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute, [64, 256]), kwargs = {})J•
pkg.torch.onnx.name_scopesÜ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'view_7']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
√
view_7
8model.encoder.encoder.layers.0.self_attn.out_proj.weight
6model.encoder.encoder.layers.0.self_attn.out_proj.biaslinear_4node_linear_4"Gemm*
beta  Ä?†*
transB†*
alpha  Ä?†*
transA †JÌ
	namespaceﬂ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/linear_4: aten.linear.defaultJô
pkg.torch.onnx.class_hierarchyˆ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.linear.default']JÅ
pkg.torch.onnx.fx_nodeÊ%linear_4 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%view_7, %p_model_encoder_encoder_layers_0_self_attn_out_proj_weight, %p_model_encoder_encoder_layers_0_self_attn_out_proj_bias), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'linear_4']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
π
linear_4
val_107view_8node_view_8"Reshape*
	allowzero†JÈ
	namespace€: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_8: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jî
pkg.torch.onnx.fx_nodez%view_8 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_4, [64, 1, 256]), kwargs = {})J•
pkg.torch.onnx.name_scopesÜ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'view_8']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
∂
view_8transpose_5node_transpose_5"	Transpose*
perm@@ @†JÔ
	namespace·: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_5: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jê
pkg.torch.onnx.fx_nodev%transpose_5 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_8, 1, 0), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.self_attn', 'transpose_5']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1510, in forward
    return attn_output.transpose(1, 0), attn_output_weights
´

add
transpose_5add_1
node_add_1"AddJç
	namespaceˇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/add_1: aten.add.TensorJ„
pkg.torch.onnx.class_hierarchy¿['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.add.Tensor']Jå
pkg.torch.onnx.fx_noder%add_1 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone_1, %clone_3), kwargs = {})Jw
pkg.torch.onnx.name_scopesY['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'add_1']JÄ
pkg.torch.onnx.stack_trace·File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
ç
add_1
+model.encoder.encoder.layers.0.norm2.weight
)model.encoder.encoder.layers.0.norm2.biaslayer_norm_2node_layer_norm_2"LayerNormalization*

stash_type†*
epsilon¨≈'7†*
axisˇˇˇˇˇˇˇˇˇ†JÎ
	namespace›: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.norm2: torch.nn.modules.normalization.LayerNorm/layer_norm_2: aten.layer_norm.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.normalization.LayerNorm', 'aten.layer_norm.default']Jı
pkg.torch.onnx.fx_node⁄%layer_norm_2 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_1, [256], %p_model_encoder_encoder_layers_0_norm2_weight, %p_model_encoder_encoder_layers_0_norm2_bias), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.norm2', 'layer_norm_2']Jê
pkg.torch.onnx.stack_traceÒFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py", line 229, in forward
    return F.layer_norm(
Â
layer_norm_2
val_110val_111node_MatMul_105"MatMulJ€
	namespaceÕ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.linear1: torch.nn.modules.linear.Linear/linear_5: aten.linear.defaultJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÒ
pkg.torch.onnx.fx_node÷%linear_5 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_2, %p_model_encoder_encoder_layers_0_linear1_weight, %p_model_encoder_encoder_layers_0_linear1_bias), kwargs = {})J•
pkg.torch.onnx.name_scopesÜ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.linear1', 'linear_5']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
Ä
val_111
+model.encoder.encoder.layers.0.linear1.biaslinear_5node_linear_5"AddJ€
	namespaceÕ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.linear1: torch.nn.modules.linear.Linear/linear_5: aten.linear.defaultJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÒ
pkg.torch.onnx.fx_node÷%linear_5 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_2, %p_model_encoder_encoder_layers_0_linear1_weight, %p_model_encoder_encoder_layers_0_linear1_bias), kwargs = {})J•
pkg.torch.onnx.name_scopesÜ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.linear1', 'linear_5']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
Ω

linear_5
val_4val_113node_Div_107"DivJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_2: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_2 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_5,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'gelu_2']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
µ

val_113val_114node_Erf_108"ErfJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_2: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_2 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_5,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'gelu_2']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
º

val_114
val_7val_116node_Add_110"AddJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_2: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_2 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_5,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'gelu_2']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
º

val_9
val_116val_118node_Mul_112"MulJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_2: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_2 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_5,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'gelu_2']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
Ω

linear_5
val_118gelu_2node_gelu_2"MulJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_2: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_2 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_5,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'gelu_2']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
⁄
gelu_2
val_119val_120node_MatMul_114"MatMulJ€
	namespaceÕ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.linear2: torch.nn.modules.linear.Linear/linear_6: aten.linear.defaultJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÏ
pkg.torch.onnx.fx_node—%linear_6 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%clone_4, %p_model_encoder_encoder_layers_0_linear2_weight, %p_model_encoder_encoder_layers_0_linear2_bias), kwargs = {})J•
pkg.torch.onnx.name_scopesÜ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.linear2', 'linear_6']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
˚
val_120
+model.encoder.encoder.layers.0.linear2.biaslinear_6node_linear_6"AddJ€
	namespaceÕ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.0.linear2: torch.nn.modules.linear.Linear/linear_6: aten.linear.defaultJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÏ
pkg.torch.onnx.fx_node—%linear_6 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%clone_4, %p_model_encoder_encoder_layers_0_linear2_weight, %p_model_encoder_encoder_layers_0_linear2_bias), kwargs = {})J•
pkg.torch.onnx.name_scopesÜ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'model.encoder.encoder.layers.0.linear2', 'linear_6']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
∂

add_1
linear_6add_2
node_add_2"AddJç
	namespaceˇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/add_2: aten.add.TensorJ„
pkg.torch.onnx.class_hierarchy¿['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.add.Tensor']Jä
pkg.torch.onnx.fx_nodep%add_2 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_1, %clone_5), kwargs = {})Jw
pkg.torch.onnx.name_scopesY['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.0', 'add_2']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
™
add_2
+model.encoder.encoder.layers.1.norm1.weight
)model.encoder.encoder.layers.1.norm1.biaslayer_norm_3node_layer_norm_3"LayerNormalization*

stash_type†*
epsilon¨≈'7†*
axisˇˇˇˇˇˇˇˇˇ†JÎ
	namespace›: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.norm1: torch.nn.modules.normalization.LayerNorm/layer_norm_3: aten.layer_norm.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.normalization.LayerNorm', 'aten.layer_norm.default']Jı
pkg.torch.onnx.fx_node⁄%layer_norm_3 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_2, [256], %p_model_encoder_encoder_layers_1_norm1_weight, %p_model_encoder_encoder_layers_1_norm1_bias), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.norm1', 'layer_norm_3']J≠
pkg.torch.onnx.stack_traceéFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 929, in forward
    self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py", line 229, in forward
    return F.layer_norm(
∂
layer_norm_3transpose_6node_transpose_6"	Transpose*
perm@@ @†JÔ
	namespace·: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_6: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jñ
pkg.torch.onnx.fx_node|%transpose_6 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%layer_norm_3, 1, 0), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'transpose_6']Jó
pkg.torch.onnx.stack_trace¯File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1454, in forward
    query = key = value = query.transpose(1, 0)
¶
transpose_6
val_123val_124node_MatMul_116"MatMulJÌ
	namespaceﬂ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/linear_7: aten.linear.defaultJô
pkg.torch.onnx.class_hierarchyˆ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.linear.default']JÑ
pkg.torch.onnx.fx_nodeÈ%linear_7 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_6, %p_model_encoder_encoder_layers_1_self_attn_in_proj_weight, %p_model_encoder_encoder_layers_1_self_attn_in_proj_bias), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'linear_7']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ã
val_124
5model.encoder.encoder.layers.1.self_attn.in_proj_biaslinear_7node_linear_7"AddJÌ
	namespaceﬂ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/linear_7: aten.linear.defaultJô
pkg.torch.onnx.class_hierarchyˆ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.linear.default']JÑ
pkg.torch.onnx.fx_nodeÈ%linear_7 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_6, %p_model_encoder_encoder_layers_1_self_attn_in_proj_weight, %p_model_encoder_encoder_layers_1_self_attn_in_proj_bias), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'linear_7']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
ª
linear_7
val_31view_9node_view_9"Reshape*
	allowzero†JÈ
	namespace€: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/view_9: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jó
pkg.torch.onnx.fx_node}%view_9 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_7, [64, 1, 3, 256]), kwargs = {})J•
pkg.torch.onnx.name_scopesÜ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'view_9']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¡
view_9
val_32unsqueeze_1node_unsqueeze_1"	UnsqueezeJÛ
	namespaceÂ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/unsqueeze_1: aten.unsqueeze.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.unsqueeze.default']Jë
pkg.torch.onnx.fx_nodew%unsqueeze_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_9, 0), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'unsqueeze_1']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
–
unsqueeze_1transpose_7node_transpose_7"	Transpose*
perm@@@@ @†JÔ
	namespace·: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_7: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jñ
pkg.torch.onnx.fx_node|%transpose_7 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%unsqueeze_1, 0, -2), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'transpose_7']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Æ
transpose_7
val_33	squeeze_1node_squeeze_1"SqueezeJÎ
	namespace›: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/squeeze_1: aten.squeeze.dimJñ
pkg.torch.onnx.class_hierarchyÛ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.squeeze.dim']Jè
pkg.torch.onnx.fx_nodeu%squeeze_1 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%transpose_7, -2), kwargs = {})J®
pkg.torch.onnx.name_scopesâ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'squeeze_1']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Æ
	squeeze_1
val_34select_3node_select_3"Gather*
axis †JÈ
	namespace€: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/select_3: aten.select.intJï
pkg.torch.onnx.class_hierarchyÚ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.select.int']Jã
pkg.torch.onnx.fx_nodeq%select_3 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%clone_6, 0, 0), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'select_3']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Æ
	squeeze_1
val_35select_4node_select_4"Gather*
axis †JÈ
	namespace€: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/select_4: aten.select.intJï
pkg.torch.onnx.class_hierarchyÚ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.select.int']Jã
pkg.torch.onnx.fx_nodeq%select_4 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%clone_6, 0, 1), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'select_4']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Æ
	squeeze_1
val_36select_5node_select_5"Gather*
axis †JÈ
	namespace€: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/select_5: aten.select.intJï
pkg.torch.onnx.class_hierarchyÚ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.select.int']Jã
pkg.torch.onnx.fx_nodeq%select_5 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%clone_6, 0, 2), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'select_5']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
º
select_3
val_41view_10node_view_10"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/view_10: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jî
pkg.torch.onnx.fx_nodez%view_10 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select_3, [64, 4, 64]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'view_10']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
√
view_10transpose_8node_transpose_8"	Transpose*
perm@@ @†JÔ
	namespace·: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_8: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jë
pkg.torch.onnx.fx_nodew%transpose_8 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_10, 0, 1), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'transpose_8']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
º
select_4
val_41view_11node_view_11"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/view_11: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jî
pkg.torch.onnx.fx_nodez%view_11 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select_4, [64, 4, 64]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'view_11']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
√
view_11transpose_9node_transpose_9"	Transpose*
perm@@ @†JÔ
	namespace·: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_9: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jë
pkg.torch.onnx.fx_nodew%transpose_9 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_11, 0, 1), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'transpose_9']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
º
select_5
val_41view_12node_view_12"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/view_12: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jî
pkg.torch.onnx.fx_nodez%view_12 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select_5, [64, 4, 64]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'view_12']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
»
view_12transpose_10node_transpose_10"	Transpose*
perm@@ @†J
	namespace‚: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_10: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jí
pkg.torch.onnx.fx_nodex%transpose_10 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_12, 0, 1), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'transpose_10']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
∆
transpose_8
val_57view_13node_view_13"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/view_13: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jõ
pkg.torch.onnx.fx_nodeÄ%view_13 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_8, [1, 4, 64, 64]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'view_13']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
»
transpose_10
val_57view_15node_view_15"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/view_15: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jú
pkg.torch.onnx.fx_nodeÅ%view_15 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_10, [1, 4, 64, 64]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'view_15']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
“
transpose_9val_182node_Transpose_174"	Transpose*
perm@ @@†Jô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_13, %view_14, %view_15), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
”
val_182
val_57val_184node_Reshape_176"Reshape*
	allowzero †Jô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_13, %view_14, %view_15), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
π
view_13
val_92val_186node_Mul_178"MulJô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_13, %view_14, %view_15), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
π
val_184
val_92val_189node_Mul_181"MulJô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_13, %view_14, %view_15), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¿
val_186
val_189val_190node_MatMul_182"MatMulJô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_13, %view_14, %view_15), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
œ
val_190val_191node_Softmax_183"Softmax*
axisˇˇˇˇˇˇˇˇˇ†Jô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_13, %view_14, %view_15), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Î
val_191
view_15scaled_dot_product_attention_1#node_scaled_dot_product_attention_1"MatMulJô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_13, %view_14, %view_15), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¯
scaled_dot_product_attention_1	permute_1node_permute_1"	Transpose*
perm@@ @@†JÔ
	namespace·: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/permute_1: aten.permute.defaultJö
pkg.torch.onnx.class_hierarchy˜['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.permute.default']J±
pkg.torch.onnx.fx_nodeñ%permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%scaled_dot_product_attention_1, [2, 0, 1, 3]), kwargs = {})J®
pkg.torch.onnx.name_scopesâ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'permute_1']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ω
	permute_1
val_102view_16node_view_16"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/view_16: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jì
pkg.torch.onnx.fx_nodey%view_16 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_1, [64, 256]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'view_16']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
≈
view_16
8model.encoder.encoder.layers.1.self_attn.out_proj.weight
6model.encoder.encoder.layers.1.self_attn.out_proj.biaslinear_8node_linear_8"Gemm*
beta  Ä?†*
transB†*
alpha  Ä?†*
transA †JÌ
	namespaceﬂ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/linear_8: aten.linear.defaultJô
pkg.torch.onnx.class_hierarchyˆ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.linear.default']JÇ
pkg.torch.onnx.fx_nodeÁ%linear_8 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%view_16, %p_model_encoder_encoder_layers_1_self_attn_out_proj_weight, %p_model_encoder_encoder_layers_1_self_attn_out_proj_bias), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'linear_8']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
æ
linear_8
val_107view_17node_view_17"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/view_17: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jï
pkg.torch.onnx.fx_node{%view_17 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_8, [64, 1, 256]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'view_17']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ω
view_17transpose_11node_transpose_11"	Transpose*
perm@@ @†J
	namespace‚: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_11: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jí
pkg.torch.onnx.fx_nodex%transpose_11 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_17, 1, 0), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.self_attn', 'transpose_11']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1510, in forward
    return attn_output.transpose(1, 0), attn_output_weights
¨

add_2
transpose_11add_3
node_add_3"AddJç
	namespaceˇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/add_3: aten.add.TensorJ„
pkg.torch.onnx.class_hierarchy¿['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.add.Tensor']Jä
pkg.torch.onnx.fx_nodep%add_3 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_2, %clone_7), kwargs = {})Jw
pkg.torch.onnx.name_scopesY['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'add_3']JÄ
pkg.torch.onnx.stack_trace·File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
ç
add_3
+model.encoder.encoder.layers.1.norm2.weight
)model.encoder.encoder.layers.1.norm2.biaslayer_norm_4node_layer_norm_4"LayerNormalization*

stash_type†*
epsilon¨≈'7†*
axisˇˇˇˇˇˇˇˇˇ†JÎ
	namespace›: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.norm2: torch.nn.modules.normalization.LayerNorm/layer_norm_4: aten.layer_norm.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.normalization.LayerNorm', 'aten.layer_norm.default']Jı
pkg.torch.onnx.fx_node⁄%layer_norm_4 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_3, [256], %p_model_encoder_encoder_layers_1_norm2_weight, %p_model_encoder_encoder_layers_1_norm2_bias), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.norm2', 'layer_norm_4']Jê
pkg.torch.onnx.stack_traceÒFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py", line 229, in forward
    return F.layer_norm(
Â
layer_norm_4
val_203val_204node_MatMul_194"MatMulJ€
	namespaceÕ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.linear1: torch.nn.modules.linear.Linear/linear_9: aten.linear.defaultJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÒ
pkg.torch.onnx.fx_node÷%linear_9 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_4, %p_model_encoder_encoder_layers_1_linear1_weight, %p_model_encoder_encoder_layers_1_linear1_bias), kwargs = {})J•
pkg.torch.onnx.name_scopesÜ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.linear1', 'linear_9']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
Ä
val_204
+model.encoder.encoder.layers.1.linear1.biaslinear_9node_linear_9"AddJ€
	namespaceÕ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.linear1: torch.nn.modules.linear.Linear/linear_9: aten.linear.defaultJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÒ
pkg.torch.onnx.fx_node÷%linear_9 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_4, %p_model_encoder_encoder_layers_1_linear1_weight, %p_model_encoder_encoder_layers_1_linear1_bias), kwargs = {})J•
pkg.torch.onnx.name_scopesÜ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.linear1', 'linear_9']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
Ω

linear_9
val_4val_206node_Div_196"DivJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_3: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_3 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_9,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'gelu_3']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
µ

val_206val_207node_Erf_197"ErfJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_3: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_3 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_9,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'gelu_3']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
º

val_207
val_7val_209node_Add_199"AddJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_3: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_3 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_9,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'gelu_3']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
º

val_9
val_209val_211node_Mul_201"MulJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_3: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_3 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_9,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'gelu_3']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
Ω

linear_9
val_211gelu_3node_gelu_3"MulJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_3: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_3 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_9,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'gelu_3']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
›
gelu_3
val_212val_213node_MatMul_203"MatMulJ‹
	namespaceŒ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.linear2: torch.nn.modules.linear.Linear/linear_10: aten.linear.defaultJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÌ
pkg.torch.onnx.fx_node“%linear_10 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%clone_8, %p_model_encoder_encoder_layers_1_linear2_weight, %p_model_encoder_encoder_layers_1_linear2_bias), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.linear2', 'linear_10']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
Ä
val_213
+model.encoder.encoder.layers.1.linear2.bias	linear_10node_linear_10"AddJ‹
	namespaceŒ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.1.linear2: torch.nn.modules.linear.Linear/linear_10: aten.linear.defaultJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÌ
pkg.torch.onnx.fx_node“%linear_10 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%clone_8, %p_model_encoder_encoder_layers_1_linear2_weight, %p_model_encoder_encoder_layers_1_linear2_bias), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'model.encoder.encoder.layers.1.linear2', 'linear_10']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
∑

add_3
	linear_10add_4
node_add_4"AddJç
	namespaceˇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/add_4: aten.add.TensorJ„
pkg.torch.onnx.class_hierarchy¿['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.add.Tensor']Jä
pkg.torch.onnx.fx_nodep%add_4 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_3, %clone_9), kwargs = {})Jw
pkg.torch.onnx.name_scopesY['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.1', 'add_4']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
™
add_4
+model.encoder.encoder.layers.2.norm1.weight
)model.encoder.encoder.layers.2.norm1.biaslayer_norm_5node_layer_norm_5"LayerNormalization*

stash_type†*
epsilon¨≈'7†*
axisˇˇˇˇˇˇˇˇˇ†JÎ
	namespace›: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.norm1: torch.nn.modules.normalization.LayerNorm/layer_norm_5: aten.layer_norm.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.normalization.LayerNorm', 'aten.layer_norm.default']Jı
pkg.torch.onnx.fx_node⁄%layer_norm_5 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_4, [256], %p_model_encoder_encoder_layers_2_norm1_weight, %p_model_encoder_encoder_layers_2_norm1_bias), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.norm1', 'layer_norm_5']J≠
pkg.torch.onnx.stack_traceéFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 929, in forward
    self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py", line 229, in forward
    return F.layer_norm(
ª
layer_norm_5transpose_12node_transpose_12"	Transpose*
perm@@ @†J
	namespace‚: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_12: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jó
pkg.torch.onnx.fx_node}%transpose_12 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%layer_norm_5, 1, 0), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'transpose_12']Jó
pkg.torch.onnx.stack_trace¯File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1454, in forward
    query = key = value = query.transpose(1, 0)
´
transpose_12
val_216val_217node_MatMul_205"MatMulJÓ
	namespace‡: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/linear_11: aten.linear.defaultJô
pkg.torch.onnx.class_hierarchyˆ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.linear.default']JÜ
pkg.torch.onnx.fx_nodeÎ%linear_11 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_12, %p_model_encoder_encoder_layers_2_self_attn_in_proj_weight, %p_model_encoder_encoder_layers_2_self_attn_in_proj_bias), kwargs = {})J®
pkg.torch.onnx.name_scopesâ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'linear_11']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
“
val_217
5model.encoder.encoder.layers.2.self_attn.in_proj_bias	linear_11node_linear_11"AddJÓ
	namespace‡: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/linear_11: aten.linear.defaultJô
pkg.torch.onnx.class_hierarchyˆ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.linear.default']JÜ
pkg.torch.onnx.fx_nodeÎ%linear_11 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_12, %p_model_encoder_encoder_layers_2_self_attn_in_proj_weight, %p_model_encoder_encoder_layers_2_self_attn_in_proj_bias), kwargs = {})J®
pkg.torch.onnx.name_scopesâ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'linear_11']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¬
	linear_11
val_31view_18node_view_18"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/view_18: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jô
pkg.torch.onnx.fx_node%view_18 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_11, [64, 1, 3, 256]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'view_18']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
√
view_18
val_32unsqueeze_2node_unsqueeze_2"	UnsqueezeJÛ
	namespaceÂ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/unsqueeze_2: aten.unsqueeze.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.unsqueeze.default']Jí
pkg.torch.onnx.fx_nodex%unsqueeze_2 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_18, 0), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'unsqueeze_2']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
’
unsqueeze_2transpose_13node_transpose_13"	Transpose*
perm@@@@ @†J
	namespace‚: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_13: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jó
pkg.torch.onnx.fx_node}%transpose_13 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%unsqueeze_2, 0, -2), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'transpose_13']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
∞
transpose_13
val_33	squeeze_2node_squeeze_2"SqueezeJÎ
	namespace›: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/squeeze_2: aten.squeeze.dimJñ
pkg.torch.onnx.class_hierarchyÛ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.squeeze.dim']Jê
pkg.torch.onnx.fx_nodev%squeeze_2 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%transpose_13, -2), kwargs = {})J®
pkg.torch.onnx.name_scopesâ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'squeeze_2']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ø
	squeeze_2
val_34select_6node_select_6"Gather*
axis †JÈ
	namespace€: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/select_6: aten.select.intJï
pkg.torch.onnx.class_hierarchyÚ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.select.int']Jå
pkg.torch.onnx.fx_noder%select_6 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%clone_10, 0, 0), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'select_6']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ø
	squeeze_2
val_35select_7node_select_7"Gather*
axis †JÈ
	namespace€: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/select_7: aten.select.intJï
pkg.torch.onnx.class_hierarchyÚ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.select.int']Jå
pkg.torch.onnx.fx_noder%select_7 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%clone_10, 0, 1), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'select_7']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ø
	squeeze_2
val_36select_8node_select_8"Gather*
axis †JÈ
	namespace€: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/select_8: aten.select.intJï
pkg.torch.onnx.class_hierarchyÚ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.select.int']Jå
pkg.torch.onnx.fx_noder%select_8 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%clone_10, 0, 2), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'select_8']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
º
select_6
val_41view_19node_view_19"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/view_19: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jî
pkg.torch.onnx.fx_nodez%view_19 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select_6, [64, 4, 64]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'view_19']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
»
view_19transpose_14node_transpose_14"	Transpose*
perm@@ @†J
	namespace‚: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_14: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jí
pkg.torch.onnx.fx_nodex%transpose_14 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_19, 0, 1), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'transpose_14']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
º
select_7
val_41view_20node_view_20"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/view_20: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jî
pkg.torch.onnx.fx_nodez%view_20 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select_7, [64, 4, 64]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'view_20']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
»
view_20transpose_15node_transpose_15"	Transpose*
perm@@ @†J
	namespace‚: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_15: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jí
pkg.torch.onnx.fx_nodex%transpose_15 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_20, 0, 1), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'transpose_15']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
º
select_8
val_41view_21node_view_21"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/view_21: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jî
pkg.torch.onnx.fx_nodez%view_21 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select_8, [64, 4, 64]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'view_21']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
»
view_21transpose_16node_transpose_16"	Transpose*
perm@@ @†J
	namespace‚: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_16: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jí
pkg.torch.onnx.fx_nodex%transpose_16 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_21, 0, 1), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'transpose_16']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
»
transpose_14
val_57view_22node_view_22"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/view_22: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jú
pkg.torch.onnx.fx_nodeÅ%view_22 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_14, [1, 4, 64, 64]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'view_22']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
»
transpose_16
val_57view_24node_view_24"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/view_24: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jú
pkg.torch.onnx.fx_nodeÅ%view_24 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_16, [1, 4, 64, 64]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'view_24']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
”
transpose_15val_275node_Transpose_263"	Transpose*
perm@ @@†Jô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_2: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_2 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'scaled_dot_product_attention_2']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
”
val_275
val_57val_277node_Reshape_265"Reshape*
	allowzero †Jô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_2: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_2 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'scaled_dot_product_attention_2']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
π
view_22
val_92val_279node_Mul_267"MulJô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_2: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_2 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'scaled_dot_product_attention_2']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
π
val_277
val_92val_282node_Mul_270"MulJô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_2: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_2 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'scaled_dot_product_attention_2']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¿
val_279
val_282val_283node_MatMul_271"MatMulJô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_2: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_2 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'scaled_dot_product_attention_2']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
œ
val_283val_284node_Softmax_272"Softmax*
axisˇˇˇˇˇˇˇˇˇ†Jô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_2: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_2 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'scaled_dot_product_attention_2']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Î
val_284
view_24scaled_dot_product_attention_2#node_scaled_dot_product_attention_2"MatMulJô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_2: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_2 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'scaled_dot_product_attention_2']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¯
scaled_dot_product_attention_2	permute_2node_permute_2"	Transpose*
perm@@ @@†JÔ
	namespace·: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/permute_2: aten.permute.defaultJö
pkg.torch.onnx.class_hierarchy˜['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.permute.default']J±
pkg.torch.onnx.fx_nodeñ%permute_2 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%scaled_dot_product_attention_2, [2, 0, 1, 3]), kwargs = {})J®
pkg.torch.onnx.name_scopesâ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'permute_2']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ω
	permute_2
val_102view_25node_view_25"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/view_25: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jì
pkg.torch.onnx.fx_nodey%view_25 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_2, [64, 256]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'view_25']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
 
view_25
8model.encoder.encoder.layers.2.self_attn.out_proj.weight
6model.encoder.encoder.layers.2.self_attn.out_proj.bias	linear_12node_linear_12"Gemm*
beta  Ä?†*
transB†*
alpha  Ä?†*
transA †JÓ
	namespace‡: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/linear_12: aten.linear.defaultJô
pkg.torch.onnx.class_hierarchyˆ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.linear.default']JÉ
pkg.torch.onnx.fx_nodeË%linear_12 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%view_25, %p_model_encoder_encoder_layers_2_self_attn_out_proj_weight, %p_model_encoder_encoder_layers_2_self_attn_out_proj_bias), kwargs = {})J®
pkg.torch.onnx.name_scopesâ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'linear_12']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¿
	linear_12
val_107view_26node_view_26"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/view_26: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jñ
pkg.torch.onnx.fx_node|%view_26 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_12, [64, 1, 256]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'view_26']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ω
view_26transpose_17node_transpose_17"	Transpose*
perm@@ @†J
	namespace‚: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_17: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jí
pkg.torch.onnx.fx_nodex%transpose_17 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_26, 1, 0), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.self_attn', 'transpose_17']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1510, in forward
    return attn_output.transpose(1, 0), attn_output_weights
≠

add_4
transpose_17add_5
node_add_5"AddJç
	namespaceˇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/add_5: aten.add.TensorJ„
pkg.torch.onnx.class_hierarchy¿['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.add.Tensor']Jã
pkg.torch.onnx.fx_nodeq%add_5 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_4, %clone_11), kwargs = {})Jw
pkg.torch.onnx.name_scopesY['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'add_5']JÄ
pkg.torch.onnx.stack_trace·File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
ç
add_5
+model.encoder.encoder.layers.2.norm2.weight
)model.encoder.encoder.layers.2.norm2.biaslayer_norm_6node_layer_norm_6"LayerNormalization*

stash_type†*
epsilon¨≈'7†*
axisˇˇˇˇˇˇˇˇˇ†JÎ
	namespace›: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.norm2: torch.nn.modules.normalization.LayerNorm/layer_norm_6: aten.layer_norm.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.normalization.LayerNorm', 'aten.layer_norm.default']Jı
pkg.torch.onnx.fx_node⁄%layer_norm_6 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_5, [256], %p_model_encoder_encoder_layers_2_norm2_weight, %p_model_encoder_encoder_layers_2_norm2_bias), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.norm2', 'layer_norm_6']Jê
pkg.torch.onnx.stack_traceÒFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py", line 229, in forward
    return F.layer_norm(
Ë
layer_norm_6
val_296val_297node_MatMul_283"MatMulJ‹
	namespaceŒ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.linear1: torch.nn.modules.linear.Linear/linear_13: aten.linear.defaultJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÚ
pkg.torch.onnx.fx_node◊%linear_13 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_6, %p_model_encoder_encoder_layers_2_linear1_weight, %p_model_encoder_encoder_layers_2_linear1_bias), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.linear1', 'linear_13']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
Ö
val_297
+model.encoder.encoder.layers.2.linear1.bias	linear_13node_linear_13"AddJ‹
	namespaceŒ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.linear1: torch.nn.modules.linear.Linear/linear_13: aten.linear.defaultJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÚ
pkg.torch.onnx.fx_node◊%linear_13 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_6, %p_model_encoder_encoder_layers_2_linear1_weight, %p_model_encoder_encoder_layers_2_linear1_bias), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.linear1', 'linear_13']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
ø

	linear_13
val_4val_299node_Div_285"DivJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_4: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_4 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_13,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'gelu_4']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
∂

val_299val_300node_Erf_286"ErfJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_4: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_4 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_13,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'gelu_4']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
Ω

val_300
val_7val_302node_Add_288"AddJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_4: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_4 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_13,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'gelu_4']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
Ω

val_9
val_302val_304node_Mul_290"MulJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_4: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_4 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_13,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'gelu_4']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
ø

	linear_13
val_304gelu_4node_gelu_4"MulJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_4: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_4 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_13,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'gelu_4']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
ﬁ
gelu_4
val_305val_306node_MatMul_292"MatMulJ‹
	namespaceŒ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.linear2: torch.nn.modules.linear.Linear/linear_14: aten.linear.defaultJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÓ
pkg.torch.onnx.fx_node”%linear_14 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%clone_12, %p_model_encoder_encoder_layers_2_linear2_weight, %p_model_encoder_encoder_layers_2_linear2_bias), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.linear2', 'linear_14']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
Å
val_306
+model.encoder.encoder.layers.2.linear2.bias	linear_14node_linear_14"AddJ‹
	namespaceŒ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.2.linear2: torch.nn.modules.linear.Linear/linear_14: aten.linear.defaultJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÓ
pkg.torch.onnx.fx_node”%linear_14 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%clone_12, %p_model_encoder_encoder_layers_2_linear2_weight, %p_model_encoder_encoder_layers_2_linear2_bias), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'model.encoder.encoder.layers.2.linear2', 'linear_14']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
∏

add_5
	linear_14add_6
node_add_6"AddJç
	namespaceˇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.2: torch.nn.modules.transformer.TransformerEncoderLayer/add_6: aten.add.TensorJ„
pkg.torch.onnx.class_hierarchy¿['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.add.Tensor']Jã
pkg.torch.onnx.fx_nodeq%add_6 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_5, %clone_13), kwargs = {})Jw
pkg.torch.onnx.name_scopesY['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.2', 'add_6']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
™
add_6
+model.encoder.encoder.layers.3.norm1.weight
)model.encoder.encoder.layers.3.norm1.biaslayer_norm_7node_layer_norm_7"LayerNormalization*

stash_type†*
epsilon¨≈'7†*
axisˇˇˇˇˇˇˇˇˇ†JÎ
	namespace›: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.norm1: torch.nn.modules.normalization.LayerNorm/layer_norm_7: aten.layer_norm.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.normalization.LayerNorm', 'aten.layer_norm.default']Jı
pkg.torch.onnx.fx_node⁄%layer_norm_7 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_6, [256], %p_model_encoder_encoder_layers_3_norm1_weight, %p_model_encoder_encoder_layers_3_norm1_bias), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.norm1', 'layer_norm_7']J≠
pkg.torch.onnx.stack_traceéFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 929, in forward
    self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py", line 229, in forward
    return F.layer_norm(
ª
layer_norm_7transpose_18node_transpose_18"	Transpose*
perm@@ @†J
	namespace‚: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_18: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jó
pkg.torch.onnx.fx_node}%transpose_18 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%layer_norm_7, 1, 0), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'transpose_18']Jó
pkg.torch.onnx.stack_trace¯File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1454, in forward
    query = key = value = query.transpose(1, 0)
´
transpose_18
val_309val_310node_MatMul_294"MatMulJÓ
	namespace‡: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/linear_15: aten.linear.defaultJô
pkg.torch.onnx.class_hierarchyˆ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.linear.default']JÜ
pkg.torch.onnx.fx_nodeÎ%linear_15 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_18, %p_model_encoder_encoder_layers_3_self_attn_in_proj_weight, %p_model_encoder_encoder_layers_3_self_attn_in_proj_bias), kwargs = {})J®
pkg.torch.onnx.name_scopesâ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'linear_15']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
“
val_310
5model.encoder.encoder.layers.3.self_attn.in_proj_bias	linear_15node_linear_15"AddJÓ
	namespace‡: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/linear_15: aten.linear.defaultJô
pkg.torch.onnx.class_hierarchyˆ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.linear.default']JÜ
pkg.torch.onnx.fx_nodeÎ%linear_15 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_18, %p_model_encoder_encoder_layers_3_self_attn_in_proj_weight, %p_model_encoder_encoder_layers_3_self_attn_in_proj_bias), kwargs = {})J®
pkg.torch.onnx.name_scopesâ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'linear_15']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¬
	linear_15
val_31view_27node_view_27"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/view_27: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jô
pkg.torch.onnx.fx_node%view_27 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_15, [64, 1, 3, 256]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'view_27']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
√
view_27
val_32unsqueeze_3node_unsqueeze_3"	UnsqueezeJÛ
	namespaceÂ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/unsqueeze_3: aten.unsqueeze.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.unsqueeze.default']Jí
pkg.torch.onnx.fx_nodex%unsqueeze_3 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_27, 0), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'unsqueeze_3']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
’
unsqueeze_3transpose_19node_transpose_19"	Transpose*
perm@@@@ @†J
	namespace‚: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_19: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jó
pkg.torch.onnx.fx_node}%transpose_19 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%unsqueeze_3, 0, -2), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'transpose_19']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
∞
transpose_19
val_33	squeeze_3node_squeeze_3"SqueezeJÎ
	namespace›: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/squeeze_3: aten.squeeze.dimJñ
pkg.torch.onnx.class_hierarchyÛ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.squeeze.dim']Jê
pkg.torch.onnx.fx_nodev%squeeze_3 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%transpose_19, -2), kwargs = {})J®
pkg.torch.onnx.name_scopesâ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'squeeze_3']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ø
	squeeze_3
val_34select_9node_select_9"Gather*
axis †JÈ
	namespace€: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/select_9: aten.select.intJï
pkg.torch.onnx.class_hierarchyÚ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.select.int']Jå
pkg.torch.onnx.fx_noder%select_9 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%clone_14, 0, 0), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'select_9']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¥
	squeeze_3
val_35	select_10node_select_10"Gather*
axis †JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/select_10: aten.select.intJï
pkg.torch.onnx.class_hierarchyÚ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.select.int']Jç
pkg.torch.onnx.fx_nodes%select_10 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%clone_14, 0, 1), kwargs = {})J®
pkg.torch.onnx.name_scopesâ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'select_10']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¥
	squeeze_3
val_36	select_11node_select_11"Gather*
axis †JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/select_11: aten.select.intJï
pkg.torch.onnx.class_hierarchyÚ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.select.int']Jç
pkg.torch.onnx.fx_nodes%select_11 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%clone_14, 0, 2), kwargs = {})J®
pkg.torch.onnx.name_scopesâ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'select_11']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
º
select_9
val_41view_28node_view_28"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/view_28: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jî
pkg.torch.onnx.fx_nodez%view_28 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select_9, [64, 4, 64]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'view_28']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
»
view_28transpose_20node_transpose_20"	Transpose*
perm@@ @†J
	namespace‚: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_20: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jí
pkg.torch.onnx.fx_nodex%transpose_20 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_28, 0, 1), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'transpose_20']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
æ
	select_10
val_41view_29node_view_29"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/view_29: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jï
pkg.torch.onnx.fx_node{%view_29 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select_10, [64, 4, 64]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'view_29']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
»
view_29transpose_21node_transpose_21"	Transpose*
perm@@ @†J
	namespace‚: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_21: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jí
pkg.torch.onnx.fx_nodex%transpose_21 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_29, 0, 1), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'transpose_21']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
æ
	select_11
val_41view_30node_view_30"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/view_30: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jï
pkg.torch.onnx.fx_node{%view_30 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select_11, [64, 4, 64]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'view_30']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
»
view_30transpose_22node_transpose_22"	Transpose*
perm@@ @†J
	namespace‚: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_22: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jí
pkg.torch.onnx.fx_nodex%transpose_22 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_30, 0, 1), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'transpose_22']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
»
transpose_20
val_57view_31node_view_31"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/view_31: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jú
pkg.torch.onnx.fx_nodeÅ%view_31 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_20, [1, 4, 64, 64]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'view_31']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
»
transpose_22
val_57view_33node_view_33"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/view_33: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jú
pkg.torch.onnx.fx_nodeÅ%view_33 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_22, [1, 4, 64, 64]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'view_33']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
”
transpose_21val_368node_Transpose_352"	Transpose*
perm@ @@†Jô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_3: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_3 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_31, %view_32, %view_33), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'scaled_dot_product_attention_3']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
”
val_368
val_57val_370node_Reshape_354"Reshape*
	allowzero †Jô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_3: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_3 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_31, %view_32, %view_33), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'scaled_dot_product_attention_3']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
π
view_31
val_92val_372node_Mul_356"MulJô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_3: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_3 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_31, %view_32, %view_33), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'scaled_dot_product_attention_3']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
π
val_370
val_92val_375node_Mul_359"MulJô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_3: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_3 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_31, %view_32, %view_33), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'scaled_dot_product_attention_3']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¿
val_372
val_375val_376node_MatMul_360"MatMulJô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_3: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_3 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_31, %view_32, %view_33), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'scaled_dot_product_attention_3']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
œ
val_376val_377node_Softmax_361"Softmax*
axisˇˇˇˇˇˇˇˇˇ†Jô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_3: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_3 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_31, %view_32, %view_33), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'scaled_dot_product_attention_3']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Î
val_377
view_33scaled_dot_product_attention_3#node_scaled_dot_product_attention_3"MatMulJô
	namespaceã: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_3: aten.scaled_dot_product_attention.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J 
pkg.torch.onnx.fx_nodeØ%scaled_dot_product_attention_3 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_31, %view_32, %view_33), kwargs = {})JΩ
pkg.torch.onnx.name_scopesû['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'scaled_dot_product_attention_3']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¯
scaled_dot_product_attention_3	permute_3node_permute_3"	Transpose*
perm@@ @@†JÔ
	namespace·: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/permute_3: aten.permute.defaultJö
pkg.torch.onnx.class_hierarchy˜['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.permute.default']J±
pkg.torch.onnx.fx_nodeñ%permute_3 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%scaled_dot_product_attention_3, [2, 0, 1, 3]), kwargs = {})J®
pkg.torch.onnx.name_scopesâ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'permute_3']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ω
	permute_3
val_102view_34node_view_34"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/view_34: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jì
pkg.torch.onnx.fx_nodey%view_34 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_3, [64, 256]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'view_34']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
 
view_34
8model.encoder.encoder.layers.3.self_attn.out_proj.weight
6model.encoder.encoder.layers.3.self_attn.out_proj.bias	linear_16node_linear_16"Gemm*
beta  Ä?†*
transB†*
alpha  Ä?†*
transA †JÓ
	namespace‡: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/linear_16: aten.linear.defaultJô
pkg.torch.onnx.class_hierarchyˆ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.linear.default']JÉ
pkg.torch.onnx.fx_nodeË%linear_16 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%view_34, %p_model_encoder_encoder_layers_3_self_attn_out_proj_weight, %p_model_encoder_encoder_layers_3_self_attn_out_proj_bias), kwargs = {})J®
pkg.torch.onnx.name_scopesâ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'linear_16']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¿
	linear_16
val_107view_35node_view_35"Reshape*
	allowzero†JÍ
	namespace‹: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/view_35: aten.view.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jñ
pkg.torch.onnx.fx_node|%view_35 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_16, [64, 1, 256]), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'view_35']JÆ
pkg.torch.onnx.stack_traceèFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ω
view_35transpose_23node_transpose_23"	Transpose*
perm@@ @†J
	namespace‚: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_23: aten.transpose.intJò
pkg.torch.onnx.class_hierarchyı['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jí
pkg.torch.onnx.fx_nodex%transpose_23 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_35, 1, 0), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.self_attn', 'transpose_23']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py", line 1510, in forward
    return attn_output.transpose(1, 0), attn_output_weights
≠

add_6
transpose_23add_7
node_add_7"AddJç
	namespaceˇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/add_7: aten.add.TensorJ„
pkg.torch.onnx.class_hierarchy¿['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.add.Tensor']Jã
pkg.torch.onnx.fx_nodeq%add_7 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_6, %clone_15), kwargs = {})Jw
pkg.torch.onnx.name_scopesY['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'add_7']JÄ
pkg.torch.onnx.stack_trace·File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 928, in forward
    x = x + self._sa_block(
ç
add_7
+model.encoder.encoder.layers.3.norm2.weight
)model.encoder.encoder.layers.3.norm2.biaslayer_norm_8node_layer_norm_8"LayerNormalization*

stash_type†*
epsilon¨≈'7†*
axisˇˇˇˇˇˇˇˇˇ†JÎ
	namespace›: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.norm2: torch.nn.modules.normalization.LayerNorm/layer_norm_8: aten.layer_norm.defaultJó
pkg.torch.onnx.class_hierarchyÙ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.normalization.LayerNorm', 'aten.layer_norm.default']Jı
pkg.torch.onnx.fx_node⁄%layer_norm_8 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_7, [256], %p_model_encoder_encoder_layers_3_norm2_weight, %p_model_encoder_encoder_layers_3_norm2_bias), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.norm2', 'layer_norm_8']Jê
pkg.torch.onnx.stack_traceÒFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py", line 229, in forward
    return F.layer_norm(
Ë
layer_norm_8
val_389val_390node_MatMul_372"MatMulJ‹
	namespaceŒ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.linear1: torch.nn.modules.linear.Linear/linear_17: aten.linear.defaultJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÚ
pkg.torch.onnx.fx_node◊%linear_17 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_8, %p_model_encoder_encoder_layers_3_linear1_weight, %p_model_encoder_encoder_layers_3_linear1_bias), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.linear1', 'linear_17']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
Ö
val_390
+model.encoder.encoder.layers.3.linear1.bias	linear_17node_linear_17"AddJ‹
	namespaceŒ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.linear1: torch.nn.modules.linear.Linear/linear_17: aten.linear.defaultJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÚ
pkg.torch.onnx.fx_node◊%linear_17 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_8, %p_model_encoder_encoder_layers_3_linear1_weight, %p_model_encoder_encoder_layers_3_linear1_bias), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.linear1', 'linear_17']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
ø

	linear_17
val_4val_392node_Div_374"DivJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_5: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_5 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_17,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'gelu_5']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
∂

val_392val_393node_Erf_375"ErfJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_5: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_5 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_17,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'gelu_5']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
Ω

val_393
val_7val_395node_Add_377"AddJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_5: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_5 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_17,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'gelu_5']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
Ω

val_9
val_395val_397node_Mul_379"MulJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_5: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_5 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_17,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'gelu_5']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
ø

	linear_17
val_397gelu_5node_gelu_5"MulJê
	namespaceÇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/gelu_5: aten.gelu.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_5 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%linear_17,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'gelu_5']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
ﬁ
gelu_5
val_398val_399node_MatMul_381"MatMulJ‹
	namespaceŒ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.linear2: torch.nn.modules.linear.Linear/linear_18: aten.linear.defaultJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÓ
pkg.torch.onnx.fx_node”%linear_18 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%clone_16, %p_model_encoder_encoder_layers_3_linear2_weight, %p_model_encoder_encoder_layers_3_linear2_bias), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.linear2', 'linear_18']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
Å
val_399
+model.encoder.encoder.layers.3.linear2.bias	linear_18node_linear_18"AddJ‹
	namespaceŒ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/model.encoder.encoder.layers.3.linear2: torch.nn.modules.linear.Linear/linear_18: aten.linear.defaultJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÓ
pkg.torch.onnx.fx_node”%linear_18 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%clone_16, %p_model_encoder_encoder_layers_3_linear2_weight, %p_model_encoder_encoder_layers_3_linear2_bias), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'model.encoder.encoder.layers.3.linear2', 'linear_18']J£
pkg.torch.onnx.stack_traceÑFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
∏

add_7
	linear_18add_8
node_add_8"AddJç
	namespaceˇ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.encoder: torch.nn.modules.transformer.TransformerEncoder/model.encoder.encoder.layers.3: torch.nn.modules.transformer.TransformerEncoderLayer/add_8: aten.add.TensorJ„
pkg.torch.onnx.class_hierarchy¿['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.add.Tensor']Jã
pkg.torch.onnx.fx_nodeq%add_8 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_7, %clone_17), kwargs = {})Jw
pkg.torch.onnx.name_scopesY['', 'model.encoder', 'model.encoder.encoder', 'model.encoder.encoder.layers.3', 'add_8']Jé
pkg.torch.onnx.stack_traceÔFile "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 61, in forward
    seq = self.encoder(seq, src_key_padding_mask=key_padding_mask)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 524, in forward
    output = mod(
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py", line 931, in forward
    x = x + self._ff_block(self.norm2(x))
Î
add_8
val_402mean	node_mean"
ReduceMean*
noop_with_empty_axes †*
keepdims †Jm
	namespace`: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/mean: aten.mean.dimJu
pkg.torch.onnx.class_hierarchyS['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'aten.mean.dim']JÇ
pkg.torch.onnx.fx_nodeh%mean : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%add_8, [1]), kwargs = {})J;
pkg.torch.onnx.name_scopes['', 'model.encoder', 'mean']JÂ
pkg.torch.onnx.stack_trace∆File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 62, in forward
    pooled = torch.mean(seq, dim=1)
Œ	
mean
model.encoder.norm.weight
model.encoder.norm.biaslayer_norm_9node_layer_norm_9"LayerNormalization*

stash_type†*
epsilon¨≈'7†*
axisˇˇˇˇˇˇˇˇˇ†JΩ
	namespaceØ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/model.encoder.norm: torch.nn.modules.normalization.LayerNorm/layer_norm_9: aten.layer_norm.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'torch.nn.modules.normalization.LayerNorm', 'aten.layer_norm.default']J–
pkg.torch.onnx.fx_nodeµ%layer_norm_9 : [num_users=2] = call_function[target=torch.ops.aten.layer_norm.default](args = (%mean, [256], %p_model_encoder_norm_weight, %p_model_encoder_norm_bias), kwargs = {})JY
pkg.torch.onnx.name_scopes;['', 'model.encoder', 'model.encoder.norm', 'layer_norm_9']JÉ
pkg.torch.onnx.stack_trace‰File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 63, in forward
    pooled = nn.functional.normalize(self.norm(pooled), dim=-1)
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py", line 229, in forward
    return F.layer_norm(
ü
layer_norm_9
val_406linalg_vector_normnode_linalg_vector_norm"ReduceL2*
noop_with_empty_axes †*
keepdims†Jé
	namespaceÄ: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/linalg_vector_norm: aten.linalg_vector_norm.defaultJá
pkg.torch.onnx.class_hierarchye['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'aten.linalg_vector_norm.default']J∂
pkg.torch.onnx.fx_nodeõ%linalg_vector_norm : [num_users=1] = call_function[target=torch.ops.aten.linalg_vector_norm.default](args = (%layer_norm_9, 2.0, [-1], True), kwargs = {})JI
pkg.torch.onnx.name_scopes+['', 'model.encoder', 'linalg_vector_norm']JÅ
pkg.torch.onnx.stack_trace‚File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 63, in forward
    pooled = nn.functional.normalize(self.norm(pooled), dim=-1)
§
linalg_vector_norm
val_408	clamp_minnode_clamp_min"ClipJ{
	namespacen: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/clamp_min: aten.clamp_min.defaultJ~
pkg.torch.onnx.class_hierarchy\['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'aten.clamp_min.default']J†
pkg.torch.onnx.fx_nodeÖ%clamp_min : [num_users=1] = call_function[target=torch.ops.aten.clamp_min.default](args = (%linalg_vector_norm, 1e-12), kwargs = {})J@
pkg.torch.onnx.name_scopes"['', 'model.encoder', 'clamp_min']JÅ
pkg.torch.onnx.stack_trace‚File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 63, in forward
    pooled = nn.functional.normalize(self.norm(pooled), dim=-1)
˛
	clamp_min
val_412expandnode_expand"ExpandJu
	namespaceh: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/expand: aten.expand.defaultJ{
pkg.torch.onnx.class_hierarchyY['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'aten.expand.default']Jì
pkg.torch.onnx.fx_nodey%expand : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%clamp_min, [1, 256]), kwargs = {})J=
pkg.torch.onnx.name_scopes['', 'model.encoder', 'expand']JÅ
pkg.torch.onnx.stack_trace‚File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 63, in forward
    pooled = nn.functional.normalize(self.norm(pooled), dim=-1)
Í
layer_norm_9
expand	embeddingnode_div"DivJn
	namespacea: __main__.EncoderWrapper/model.encoder: __main__.TemporalTransformerEncoder/div: aten.div.TensorJw
pkg.torch.onnx.class_hierarchyU['__main__.EncoderWrapper', '__main__.TemporalTransformerEncoder', 'aten.div.Tensor']Jé
pkg.torch.onnx.fx_nodet%div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%layer_norm_9, %expand), kwargs = {})J:
pkg.torch.onnx.name_scopes['', 'model.encoder', 'div']JÅ
pkg.torch.onnx.stack_trace‚File "/tmp/ipython-input-3704775897.py", line 13, in forward
    return self.model.encode_visual(x)
  File "/tmp/ipython-input-3079789015.py", line 63, in forward
    pooled = nn.functional.normalize(self.norm(pooled), dim=-1)
main_graph*iúBmodel.projector.norm.weightj"
locationsign_encoder.onnx.dataj
offset37120j
length2160p*gúBmodel.projector.norm.biasj"
locationsign_encoder.onnx.dataj
offset39280j
length2160p*a†Bmodel.projector.fc1.biasj"
locationsign_encoder.onnx.dataj
offset0j
length640p*c†Bmodel.projector.fc2.biasj"
locationsign_encoder.onnx.dataj
offset640j
length640p*jÄBmodel.encoder.input_proj.biasj"
locationsign_encoder.onnx.dataj
offset1280j
length1024p*ÉÄB5model.encoder.encoder.layers.0.self_attn.in_proj_biasj"
locationsign_encoder.onnx.dataj
offset41440j
length3072p*ÉÄB6model.encoder.encoder.layers.0.self_attn.out_proj.biasj"
locationsign_encoder.onnx.dataj
offset2304j
length1024p*yÄB+model.encoder.encoder.layers.0.linear1.biasj"
locationsign_encoder.onnx.dataj
offset28928j
length2048p*xÄB+model.encoder.encoder.layers.0.linear2.biasj"
locationsign_encoder.onnx.dataj
offset3328j
length1024p*xÄB+model.encoder.encoder.layers.0.norm1.weightj"
locationsign_encoder.onnx.dataj
offset4352j
length1024p*vÄB)model.encoder.encoder.layers.0.norm1.biasj"
locationsign_encoder.onnx.dataj
offset5376j
length1024p*xÄB+model.encoder.encoder.layers.0.norm2.weightj"
locationsign_encoder.onnx.dataj
offset6400j
length1024p*vÄB)model.encoder.encoder.layers.0.norm2.biasj"
locationsign_encoder.onnx.dataj
offset7424j
length1024p*ÉÄB5model.encoder.encoder.layers.1.self_attn.in_proj_biasj"
locationsign_encoder.onnx.dataj
offset44512j
length3072p*ÉÄB6model.encoder.encoder.layers.1.self_attn.out_proj.biasj"
locationsign_encoder.onnx.dataj
offset8448j
length1024p*yÄB+model.encoder.encoder.layers.1.linear1.biasj"
locationsign_encoder.onnx.dataj
offset30976j
length2048p*xÄB+model.encoder.encoder.layers.1.linear2.biasj"
locationsign_encoder.onnx.dataj
offset9472j
length1024p*yÄB+model.encoder.encoder.layers.1.norm1.weightj"
locationsign_encoder.onnx.dataj
offset10496j
length1024p*wÄB)model.encoder.encoder.layers.1.norm1.biasj"
locationsign_encoder.onnx.dataj
offset11520j
length1024p*yÄB+model.encoder.encoder.layers.1.norm2.weightj"
locationsign_encoder.onnx.dataj
offset12544j
length1024p*wÄB)model.encoder.encoder.layers.1.norm2.biasj"
locationsign_encoder.onnx.dataj
offset13568j
length1024p*ÉÄB5model.encoder.encoder.layers.2.self_attn.in_proj_biasj"
locationsign_encoder.onnx.dataj
offset47584j
length3072p*ÑÄB6model.encoder.encoder.layers.2.self_attn.out_proj.biasj"
locationsign_encoder.onnx.dataj
offset14592j
length1024p*yÄB+model.encoder.encoder.layers.2.linear1.biasj"
locationsign_encoder.onnx.dataj
offset33024j
length2048p*yÄB+model.encoder.encoder.layers.2.linear2.biasj"
locationsign_encoder.onnx.dataj
offset15616j
length1024p*yÄB+model.encoder.encoder.layers.2.norm1.weightj"
locationsign_encoder.onnx.dataj
offset16640j
length1024p*wÄB)model.encoder.encoder.layers.2.norm1.biasj"
locationsign_encoder.onnx.dataj
offset17664j
length1024p*yÄB+model.encoder.encoder.layers.2.norm2.weightj"
locationsign_encoder.onnx.dataj
offset18688j
length1024p*wÄB)model.encoder.encoder.layers.2.norm2.biasj"
locationsign_encoder.onnx.dataj
offset19712j
length1024p*ÉÄB5model.encoder.encoder.layers.3.self_attn.in_proj_biasj"
locationsign_encoder.onnx.dataj
offset50656j
length3072p*ÑÄB6model.encoder.encoder.layers.3.self_attn.out_proj.biasj"
locationsign_encoder.onnx.dataj
offset20736j
length1024p*yÄB+model.encoder.encoder.layers.3.linear1.biasj"
locationsign_encoder.onnx.dataj
offset35072j
length2048p*yÄB+model.encoder.encoder.layers.3.linear2.biasj"
locationsign_encoder.onnx.dataj
offset21760j
length1024p*yÄB+model.encoder.encoder.layers.3.norm1.weightj"
locationsign_encoder.onnx.dataj
offset22784j
length1024p*wÄB)model.encoder.encoder.layers.3.norm1.biasj"
locationsign_encoder.onnx.dataj
offset23808j
length1024p*yÄB+model.encoder.encoder.layers.3.norm2.weightj"
locationsign_encoder.onnx.dataj
offset24832j
length1024p*wÄB)model.encoder.encoder.layers.3.norm2.biasj"
locationsign_encoder.onnx.dataj
offset25856j
length1024p*gÄBmodel.encoder.norm.weightj"
locationsign_encoder.onnx.dataj
offset26880j
length1024p*eÄBmodel.encoder.norm.biasj"
locationsign_encoder.onnx.dataj
offset27904j
length1024p*åÄÄB8model.encoder.encoder.layers.0.self_attn.out_proj.weightj"
locationsign_encoder.onnx.dataj
offset385504j
length262144p*åÄÄB8model.encoder.encoder.layers.1.self_attn.out_proj.weightj"
locationsign_encoder.onnx.dataj
offset647648j
length262144p*åÄÄB8model.encoder.encoder.layers.2.self_attn.out_proj.weightj"
locationsign_encoder.onnx.dataj
offset909792j
length262144p*çÄÄB8model.encoder.encoder.layers.3.self_attn.out_proj.weightj"
locationsign_encoder.onnx.dataj
offset1171936j
length262144p*o@ÄBmodel.encoder.pos_encoder.pej"
locationsign_encoder.onnx.dataj
offset53728j
length65536p*Zú†Bval_2j"
locationsign_encoder.onnx.dataj
offset1434080j
length345600p*Z††Bval_11j"
locationsign_encoder.onnx.dataj
offset119264j
length102400p*Z†ÄBval_20j"
locationsign_encoder.onnx.dataj
offset221664j
length163840p*[ÄÄBval_24j"
locationsign_encoder.onnx.dataj
offset5973984j
length786432p*.Bval_31J @                            *&Bval_41J@              @       *.Bval_57J               @       @       *Bval_92JÛµ>*Bval_102J@              *'Bval_107J@                     *\ÄÄBval_110j"
locationsign_encoder.onnx.dataj
offset1779680j
length524288p*\ÄÄBval_119j"
locationsign_encoder.onnx.dataj
offset2303968j
length524288p*\ÄÄBval_123j"
locationsign_encoder.onnx.dataj
offset6760416j
length786432p*\ÄÄBval_203j"
locationsign_encoder.onnx.dataj
offset2828256j
length524288p*\ÄÄBval_212j"
locationsign_encoder.onnx.dataj
offset3352544j
length524288p*\ÄÄBval_216j"
locationsign_encoder.onnx.dataj
offset7546848j
length786432p*\ÄÄBval_296j"
locationsign_encoder.onnx.dataj
offset3876832j
length524288p*\ÄÄBval_305j"
locationsign_encoder.onnx.dataj
offset4401120j
length524288p*\ÄÄBval_309j"
locationsign_encoder.onnx.dataj
offset8333280j
length786432p*\ÄÄBval_389j"
locationsign_encoder.onnx.dataj
offset4925408j
length524288p*\ÄÄBval_398j"
locationsign_encoder.onnx.dataj
offset5449696j
length524288p*Bval_402J       *Bval_406Jˇˇˇˇˇˇˇˇ*Bval_408JÃºå+*Bval_412J              *Bval_4JÛµ?*Bval_7J  Ä?*Bval_9J   ?*Bval_32J        *Bval_33J˛ˇˇˇˇˇˇˇ*Bval_34J        *Bval_35J       *Bval_36J       Z≈
sequence


@
ú"=
/pkg.torch.export.graph_signature.InputSpec.kind
USER_INPUT"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"&
!pkg.torch.onnx.original_node_namexbá
	embedding
	

Ä"?
0pkg.torch.export.graph_signature.OutputSpec.kindUSER_OUTPUT"(
!pkg.torch.onnx.original_node_namedivjÎ
model.projector.norm.weight
	
ú"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"B
!pkg.torch.onnx.original_node_namep_model_projector_norm_weightjÁ
model.projector.norm.bias
	
ú"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"@
!pkg.torch.onnx.original_node_namep_model_projector_norm_biasjÂ
model.projector.fc1.bias
	
†"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"?
!pkg.torch.onnx.original_node_namep_model_projector_fc1_biasjÂ
model.projector.fc2.bias
	
†"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"?
!pkg.torch.onnx.original_node_namep_model_projector_fc2_biasjÔ
model.encoder.input_proj.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"D
!pkg.torch.onnx.original_node_namep_model_encoder_input_proj_biasjü
5model.encoder.encoder.layers.0.self_attn.in_proj_bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"\
!pkg.torch.onnx.original_node_name7p_model_encoder_encoder_layers_0_self_attn_in_proj_biasj°
6model.encoder.encoder.layers.0.self_attn.out_proj.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"]
!pkg.torch.onnx.original_node_name8p_model_encoder_encoder_layers_0_self_attn_out_proj_biasjã
+model.encoder.encoder.layers.0.linear1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"R
!pkg.torch.onnx.original_node_name-p_model_encoder_encoder_layers_0_linear1_biasjã
+model.encoder.encoder.layers.0.linear2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"R
!pkg.torch.onnx.original_node_name-p_model_encoder_encoder_layers_0_linear2_biasjã
+model.encoder.encoder.layers.0.norm1.weight
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"R
!pkg.torch.onnx.original_node_name-p_model_encoder_encoder_layers_0_norm1_weightjá
)model.encoder.encoder.layers.0.norm1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"P
!pkg.torch.onnx.original_node_name+p_model_encoder_encoder_layers_0_norm1_biasjã
+model.encoder.encoder.layers.0.norm2.weight
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"R
!pkg.torch.onnx.original_node_name-p_model_encoder_encoder_layers_0_norm2_weightjá
)model.encoder.encoder.layers.0.norm2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"P
!pkg.torch.onnx.original_node_name+p_model_encoder_encoder_layers_0_norm2_biasjü
5model.encoder.encoder.layers.1.self_attn.in_proj_bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"\
!pkg.torch.onnx.original_node_name7p_model_encoder_encoder_layers_1_self_attn_in_proj_biasj°
6model.encoder.encoder.layers.1.self_attn.out_proj.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"]
!pkg.torch.onnx.original_node_name8p_model_encoder_encoder_layers_1_self_attn_out_proj_biasjã
+model.encoder.encoder.layers.1.linear1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"R
!pkg.torch.onnx.original_node_name-p_model_encoder_encoder_layers_1_linear1_biasjã
+model.encoder.encoder.layers.1.linear2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"R
!pkg.torch.onnx.original_node_name-p_model_encoder_encoder_layers_1_linear2_biasjã
+model.encoder.encoder.layers.1.norm1.weight
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"R
!pkg.torch.onnx.original_node_name-p_model_encoder_encoder_layers_1_norm1_weightjá
)model.encoder.encoder.layers.1.norm1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"P
!pkg.torch.onnx.original_node_name+p_model_encoder_encoder_layers_1_norm1_biasjã
+model.encoder.encoder.layers.1.norm2.weight
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"R
!pkg.torch.onnx.original_node_name-p_model_encoder_encoder_layers_1_norm2_weightjá
)model.encoder.encoder.layers.1.norm2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"P
!pkg.torch.onnx.original_node_name+p_model_encoder_encoder_layers_1_norm2_biasjü
5model.encoder.encoder.layers.2.self_attn.in_proj_bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"\
!pkg.torch.onnx.original_node_name7p_model_encoder_encoder_layers_2_self_attn_in_proj_biasj°
6model.encoder.encoder.layers.2.self_attn.out_proj.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"]
!pkg.torch.onnx.original_node_name8p_model_encoder_encoder_layers_2_self_attn_out_proj_biasjã
+model.encoder.encoder.layers.2.linear1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"R
!pkg.torch.onnx.original_node_name-p_model_encoder_encoder_layers_2_linear1_biasjã
+model.encoder.encoder.layers.2.linear2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"R
!pkg.torch.onnx.original_node_name-p_model_encoder_encoder_layers_2_linear2_biasjã
+model.encoder.encoder.layers.2.norm1.weight
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"R
!pkg.torch.onnx.original_node_name-p_model_encoder_encoder_layers_2_norm1_weightjá
)model.encoder.encoder.layers.2.norm1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"P
!pkg.torch.onnx.original_node_name+p_model_encoder_encoder_layers_2_norm1_biasjã
+model.encoder.encoder.layers.2.norm2.weight
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"R
!pkg.torch.onnx.original_node_name-p_model_encoder_encoder_layers_2_norm2_weightjá
)model.encoder.encoder.layers.2.norm2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"P
!pkg.torch.onnx.original_node_name+p_model_encoder_encoder_layers_2_norm2_biasjü
5model.encoder.encoder.layers.3.self_attn.in_proj_bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"\
!pkg.torch.onnx.original_node_name7p_model_encoder_encoder_layers_3_self_attn_in_proj_biasj°
6model.encoder.encoder.layers.3.self_attn.out_proj.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"]
!pkg.torch.onnx.original_node_name8p_model_encoder_encoder_layers_3_self_attn_out_proj_biasjã
+model.encoder.encoder.layers.3.linear1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"R
!pkg.torch.onnx.original_node_name-p_model_encoder_encoder_layers_3_linear1_biasjã
+model.encoder.encoder.layers.3.linear2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"R
!pkg.torch.onnx.original_node_name-p_model_encoder_encoder_layers_3_linear2_biasjã
+model.encoder.encoder.layers.3.norm1.weight
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"R
!pkg.torch.onnx.original_node_name-p_model_encoder_encoder_layers_3_norm1_weightjá
)model.encoder.encoder.layers.3.norm1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"P
!pkg.torch.onnx.original_node_name+p_model_encoder_encoder_layers_3_norm1_biasjã
+model.encoder.encoder.layers.3.norm2.weight
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"R
!pkg.torch.onnx.original_node_name-p_model_encoder_encoder_layers_3_norm2_weightjá
)model.encoder.encoder.layers.3.norm2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"P
!pkg.torch.onnx.original_node_name+p_model_encoder_encoder_layers_3_norm2_biasjÁ
model.encoder.norm.weight
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"@
!pkg.torch.onnx.original_node_namep_model_encoder_norm_weightj„
model.encoder.norm.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone">
!pkg.torch.onnx.original_node_namep_model_encoder_norm_biasj™
8model.encoder.encoder.layers.0.self_attn.out_proj.weight


Ä
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"_
!pkg.torch.onnx.original_node_name:p_model_encoder_encoder_layers_0_self_attn_out_proj_weightj™
8model.encoder.encoder.layers.1.self_attn.out_proj.weight


Ä
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"_
!pkg.torch.onnx.original_node_name:p_model_encoder_encoder_layers_1_self_attn_out_proj_weightj™
8model.encoder.encoder.layers.2.self_attn.out_proj.weight


Ä
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"_
!pkg.torch.onnx.original_node_name:p_model_encoder_encoder_layers_2_self_attn_out_proj_weightj™
8model.encoder.encoder.layers.3.self_attn.out_proj.weight


Ä
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"_
!pkg.torch.onnx.original_node_name:p_model_encoder_encoder_layers_3_self_attn_out_proj_weightjÚ
model.encoder.pos_encoder.pe


@
Ä"9
/pkg.torch.export.graph_signature.InputSpec.kindBUFFER"=
5pkg.torch.export.graph_signature.InputSpec.persistentTrue"C
!pkg.torch.onnx.original_node_nameb_model_encoder_pos_encoder_peja
val_2


ú
†"F
$pkg.onnxscript.optimizer.folded_from['model.projector.fc1.weight']jb
val_11


†
†"F
$pkg.onnxscript.optimizer.folded_from['model.projector.fc2.weight']jg
val_20


†
Ä"K
$pkg.onnxscript.optimizer.folded_from#['model.encoder.input_proj.weight']j
val_24


Ä
Ä"c
$pkg.onnxscript.optimizer.folded_from;['model.encoder.encoder.layers.0.self_attn.in_proj_weight']jf
val_31


"P
$pkg.onnxscript.optimizer.folded_from(['val_27', 'val_28', 'val_29', 'val_30']j\
val_41


"F
$pkg.onnxscript.optimizer.folded_from['val_38', 'val_39', 'val_40']jf
val_57


"P
$pkg.onnxscript.optimizer.folded_from(['val_53', 'val_54', 'val_55', 'val_56']j£
val_92


"å
$pkg.onnxscript.optimizer.folded_fromd['val_70', 'val_71', 'val_72', 'val_73', 'val_74', 'val_75', 'val_76', 'val_77', 'val_78', 'view_4']jU
val_102


">
$pkg.onnxscript.optimizer.folded_from['val_100', 'val_101']j`
val_107


"I
$pkg.onnxscript.optimizer.folded_from!['val_104', 'val_105', 'val_106']jv
val_110


Ä
Ä"Y
$pkg.onnxscript.optimizer.folded_from1['model.encoder.encoder.layers.0.linear1.weight']jv
val_119


Ä
Ä"Y
$pkg.onnxscript.optimizer.folded_from1['model.encoder.encoder.layers.0.linear2.weight']jÄ
val_123


Ä
Ä"c
$pkg.onnxscript.optimizer.folded_from;['model.encoder.encoder.layers.1.self_attn.in_proj_weight']jv
val_203


Ä
Ä"Y
$pkg.onnxscript.optimizer.folded_from1['model.encoder.encoder.layers.1.linear1.weight']jv
val_212


Ä
Ä"Y
$pkg.onnxscript.optimizer.folded_from1['model.encoder.encoder.layers.1.linear2.weight']jÄ
val_216


Ä
Ä"c
$pkg.onnxscript.optimizer.folded_from;['model.encoder.encoder.layers.2.self_attn.in_proj_weight']jv
val_296


Ä
Ä"Y
$pkg.onnxscript.optimizer.folded_from1['model.encoder.encoder.layers.2.linear1.weight']jv
val_305


Ä
Ä"Y
$pkg.onnxscript.optimizer.folded_from1['model.encoder.encoder.layers.2.linear2.weight']jÄ
val_309


Ä
Ä"c
$pkg.onnxscript.optimizer.folded_from;['model.encoder.encoder.layers.3.self_attn.in_proj_weight']jv
val_389


Ä
Ä"Y
$pkg.onnxscript.optimizer.folded_from1['model.encoder.encoder.layers.3.linear1.weight']jv
val_398


Ä
Ä"Y
$pkg.onnxscript.optimizer.folded_from1['model.encoder.encoder.layers.3.linear2.weight']jU
val_402


">
$pkg.onnxscript.optimizer.folded_from['val_400', 'val_401']jT
val_406


"=
$pkg.onnxscript.optimizer.folded_from['val_405', 'val_81']jF
val_408
 "3
$pkg.onnxscript.optimizer.folded_from['val_407']jU
val_412


">
$pkg.onnxscript.optimizer.folded_from['val_410', 'val_411']j
val_4
 j
val_7
 j
val_9
 j
val_32


j
val_33


j
val_34
 j
val_35
 j
val_36
 j!

layer_norm


@
új
val_3


@
†j
linear


@
†j
val_5


@
†j
val_6


@
†j
val_8


@
†j
val_10


@
†j
gelu


@
†j
val_12


@
†j
linear_1


@
†j
val_14


@
†j
val_15


@
†j
val_17


@
†j
val_19


@
†j
gelu_1


@
†j
val_21


@
Äj
linear_2


@
Äj
add


@
Äj#
layer_norm_1


@
Äj 
	transpose

@

Äj
val_25

@

Äj
linear_3

@

Äj
view

@


Äj(
	unsqueeze


@


Äj*
transpose_1


@


Äj"
squeeze


@

Äj
select

@

Äj
select_1

@

Äj
select_2

@

Äj
view_1

@

@j!
transpose_2


@
@j
view_2

@

@j!
transpose_3


@
@j
view_3

@

@j!
transpose_4


@
@j 
view_4



@
@j 
view_6



@
@j
val_89


@
@j 
val_91



@
@j 
val_93



@
@j 
val_96



@
@j 
val_97



@
@j 
val_98



@
@j6
scaled_dot_product_attention



@
@j!
permute

@


@j
view_7
	
@
Äj
linear_4
	
@
Äj
view_8

@

Äj"
transpose_5


@
Äj
add_1


@
Äj#
layer_norm_2


@
Äj
val_111


@
Äj
linear_5


@
Äj
val_113


@
Äj
val_114


@
Äj
val_116


@
Äj
val_118


@
Äj
gelu_2


@
Äj
val_120


@
Äj
linear_6


@
Äj
add_2


@
Äj#
layer_norm_3


@
Äj"
transpose_6

@

Äj
val_124

@

Äj
linear_7

@

Äj!
view_9

@


Äj*
unsqueeze_1


@


Äj*
transpose_7


@


Äj$
	squeeze_1


@

Äj
select_3

@

Äj
select_4

@

Äj
select_5

@

Äj
view_10

@

@j!
transpose_8


@
@j
view_11

@

@j!
transpose_9


@
@j
view_12

@

@j"
transpose_10


@
@j!
view_13



@
@j!
view_15



@
@j
val_182


@
@j!
val_184



@
@j!
val_186



@
@j!
val_189



@
@j!
val_190



@
@j!
val_191



@
@j8
scaled_dot_product_attention_1



@
@j#
	permute_1

@


@j
view_16
	
@
Äj
linear_8
	
@
Äj
view_17

@

Äj#
transpose_11


@
Äj
add_3


@
Äj#
layer_norm_4


@
Äj
val_204


@
Äj
linear_9


@
Äj
val_206


@
Äj
val_207


@
Äj
val_209


@
Äj
val_211


@
Äj
gelu_3


@
Äj
val_213


@
Äj 
	linear_10


@
Äj
add_4


@
Äj#
layer_norm_5


@
Äj#
transpose_12

@

Äj
val_217

@

Äj 
	linear_11

@

Äj"
view_18

@


Äj*
unsqueeze_2


@


Äj+
transpose_13


@


Äj$
	squeeze_2


@

Äj
select_6

@

Äj
select_7

@

Äj
select_8

@

Äj
view_19

@

@j"
transpose_14


@
@j
view_20

@

@j"
transpose_15


@
@j
view_21

@

@j"
transpose_16


@
@j!
view_22



@
@j!
view_24



@
@j
val_275


@
@j!
val_277



@
@j!
val_279



@
@j!
val_282



@
@j!
val_283



@
@j!
val_284



@
@j8
scaled_dot_product_attention_2



@
@j#
	permute_2

@


@j
view_25
	
@
Äj
	linear_12
	
@
Äj
view_26

@

Äj#
transpose_17


@
Äj
add_5


@
Äj#
layer_norm_6


@
Äj
val_297


@
Äj 
	linear_13


@
Äj
val_299


@
Äj
val_300


@
Äj
val_302


@
Äj
val_304


@
Äj
gelu_4


@
Äj
val_306


@
Äj 
	linear_14


@
Äj
add_6


@
Äj#
layer_norm_7


@
Äj#
transpose_18

@

Äj
val_310

@

Äj 
	linear_15

@

Äj"
view_27

@


Äj*
unsqueeze_3


@


Äj+
transpose_19


@


Äj$
	squeeze_3


@

Äj
select_9

@

Äj 
	select_10

@

Äj 
	select_11

@

Äj
view_28

@

@j"
transpose_20


@
@j
view_29

@

@j"
transpose_21


@
@j
view_30

@

@j"
transpose_22


@
@j!
view_31



@
@j!
view_33



@
@j
val_368


@
@j!
val_370



@
@j!
val_372



@
@j!
val_375



@
@j!
val_376



@
@j!
val_377



@
@j8
scaled_dot_product_attention_3



@
@j#
	permute_3

@


@j
view_34
	
@
Äj
	linear_16
	
@
Äj
view_35

@

Äj#
transpose_23


@
Äj
add_7


@
Äj#
layer_norm_8


@
Äj
val_390


@
Äj 
	linear_17


@
Äj
val_392


@
Äj
val_393


@
Äj
val_395


@
Äj
val_397


@
Äj
gelu_5


@
Äj
val_399


@
Äj 
	linear_18


@
Äj
add_8


@
Äj
mean
	

Äj
layer_norm_9
	

Äj$
linalg_vector_norm


j
	clamp_min


j
expand
	

ÄÇ”W
0pkg.torch.export.ExportedProgram.graph_signatureûW
# inputs
p_model_mask_token: PARAMETER target='model.mask_token'
p_model_projector_norm_weight: PARAMETER target='model.projector.norm.weight'
p_model_projector_norm_bias: PARAMETER target='model.projector.norm.bias'
p_model_projector_fc1_weight: PARAMETER target='model.projector.fc1.weight'
p_model_projector_fc1_bias: PARAMETER target='model.projector.fc1.bias'
p_model_projector_fc2_weight: PARAMETER target='model.projector.fc2.weight'
p_model_projector_fc2_bias: PARAMETER target='model.projector.fc2.bias'
p_model_encoder_input_proj_weight: PARAMETER target='model.encoder.input_proj.weight'
p_model_encoder_input_proj_bias: PARAMETER target='model.encoder.input_proj.bias'
p_model_encoder_encoder_layers_0_self_attn_in_proj_weight: PARAMETER target='model.encoder.encoder.layers.0.self_attn.in_proj_weight'
p_model_encoder_encoder_layers_0_self_attn_in_proj_bias: PARAMETER target='model.encoder.encoder.layers.0.self_attn.in_proj_bias'
p_model_encoder_encoder_layers_0_self_attn_out_proj_weight: PARAMETER target='model.encoder.encoder.layers.0.self_attn.out_proj.weight'
p_model_encoder_encoder_layers_0_self_attn_out_proj_bias: PARAMETER target='model.encoder.encoder.layers.0.self_attn.out_proj.bias'
p_model_encoder_encoder_layers_0_linear1_weight: PARAMETER target='model.encoder.encoder.layers.0.linear1.weight'
p_model_encoder_encoder_layers_0_linear1_bias: PARAMETER target='model.encoder.encoder.layers.0.linear1.bias'
p_model_encoder_encoder_layers_0_linear2_weight: PARAMETER target='model.encoder.encoder.layers.0.linear2.weight'
p_model_encoder_encoder_layers_0_linear2_bias: PARAMETER target='model.encoder.encoder.layers.0.linear2.bias'
p_model_encoder_encoder_layers_0_norm1_weight: PARAMETER target='model.encoder.encoder.layers.0.norm1.weight'
p_model_encoder_encoder_layers_0_norm1_bias: PARAMETER target='model.encoder.encoder.layers.0.norm1.bias'
p_model_encoder_encoder_layers_0_norm2_weight: PARAMETER target='model.encoder.encoder.layers.0.norm2.weight'
p_model_encoder_encoder_layers_0_norm2_bias: PARAMETER target='model.encoder.encoder.layers.0.norm2.bias'
p_model_encoder_encoder_layers_1_self_attn_in_proj_weight: PARAMETER target='model.encoder.encoder.layers.1.self_attn.in_proj_weight'
p_model_encoder_encoder_layers_1_self_attn_in_proj_bias: PARAMETER target='model.encoder.encoder.layers.1.self_attn.in_proj_bias'
p_model_encoder_encoder_layers_1_self_attn_out_proj_weight: PARAMETER target='model.encoder.encoder.layers.1.self_attn.out_proj.weight'
p_model_encoder_encoder_layers_1_self_attn_out_proj_bias: PARAMETER target='model.encoder.encoder.layers.1.self_attn.out_proj.bias'
p_model_encoder_encoder_layers_1_linear1_weight: PARAMETER target='model.encoder.encoder.layers.1.linear1.weight'
p_model_encoder_encoder_layers_1_linear1_bias: PARAMETER target='model.encoder.encoder.layers.1.linear1.bias'
p_model_encoder_encoder_layers_1_linear2_weight: PARAMETER target='model.encoder.encoder.layers.1.linear2.weight'
p_model_encoder_encoder_layers_1_linear2_bias: PARAMETER target='model.encoder.encoder.layers.1.linear2.bias'
p_model_encoder_encoder_layers_1_norm1_weight: PARAMETER target='model.encoder.encoder.layers.1.norm1.weight'
p_model_encoder_encoder_layers_1_norm1_bias: PARAMETER target='model.encoder.encoder.layers.1.norm1.bias'
p_model_encoder_encoder_layers_1_norm2_weight: PARAMETER target='model.encoder.encoder.layers.1.norm2.weight'
p_model_encoder_encoder_layers_1_norm2_bias: PARAMETER target='model.encoder.encoder.layers.1.norm2.bias'
p_model_encoder_encoder_layers_2_self_attn_in_proj_weight: PARAMETER target='model.encoder.encoder.layers.2.self_attn.in_proj_weight'
p_model_encoder_encoder_layers_2_self_attn_in_proj_bias: PARAMETER target='model.encoder.encoder.layers.2.self_attn.in_proj_bias'
p_model_encoder_encoder_layers_2_self_attn_out_proj_weight: PARAMETER target='model.encoder.encoder.layers.2.self_attn.out_proj.weight'
p_model_encoder_encoder_layers_2_self_attn_out_proj_bias: PARAMETER target='model.encoder.encoder.layers.2.self_attn.out_proj.bias'
p_model_encoder_encoder_layers_2_linear1_weight: PARAMETER target='model.encoder.encoder.layers.2.linear1.weight'
p_model_encoder_encoder_layers_2_linear1_bias: PARAMETER target='model.encoder.encoder.layers.2.linear1.bias'
p_model_encoder_encoder_layers_2_linear2_weight: PARAMETER target='model.encoder.encoder.layers.2.linear2.weight'
p_model_encoder_encoder_layers_2_linear2_bias: PARAMETER target='model.encoder.encoder.layers.2.linear2.bias'
p_model_encoder_encoder_layers_2_norm1_weight: PARAMETER target='model.encoder.encoder.layers.2.norm1.weight'
p_model_encoder_encoder_layers_2_norm1_bias: PARAMETER target='model.encoder.encoder.layers.2.norm1.bias'
p_model_encoder_encoder_layers_2_norm2_weight: PARAMETER target='model.encoder.encoder.layers.2.norm2.weight'
p_model_encoder_encoder_layers_2_norm2_bias: PARAMETER target='model.encoder.encoder.layers.2.norm2.bias'
p_model_encoder_encoder_layers_3_self_attn_in_proj_weight: PARAMETER target='model.encoder.encoder.layers.3.self_attn.in_proj_weight'
p_model_encoder_encoder_layers_3_self_attn_in_proj_bias: PARAMETER target='model.encoder.encoder.layers.3.self_attn.in_proj_bias'
p_model_encoder_encoder_layers_3_self_attn_out_proj_weight: PARAMETER target='model.encoder.encoder.layers.3.self_attn.out_proj.weight'
p_model_encoder_encoder_layers_3_self_attn_out_proj_bias: PARAMETER target='model.encoder.encoder.layers.3.self_attn.out_proj.bias'
p_model_encoder_encoder_layers_3_linear1_weight: PARAMETER target='model.encoder.encoder.layers.3.linear1.weight'
p_model_encoder_encoder_layers_3_linear1_bias: PARAMETER target='model.encoder.encoder.layers.3.linear1.bias'
p_model_encoder_encoder_layers_3_linear2_weight: PARAMETER target='model.encoder.encoder.layers.3.linear2.weight'
p_model_encoder_encoder_layers_3_linear2_bias: PARAMETER target='model.encoder.encoder.layers.3.linear2.bias'
p_model_encoder_encoder_layers_3_norm1_weight: PARAMETER target='model.encoder.encoder.layers.3.norm1.weight'
p_model_encoder_encoder_layers_3_norm1_bias: PARAMETER target='model.encoder.encoder.layers.3.norm1.bias'
p_model_encoder_encoder_layers_3_norm2_weight: PARAMETER target='model.encoder.encoder.layers.3.norm2.weight'
p_model_encoder_encoder_layers_3_norm2_bias: PARAMETER target='model.encoder.encoder.layers.3.norm2.bias'
p_model_encoder_norm_weight: PARAMETER target='model.encoder.norm.weight'
p_model_encoder_norm_bias: PARAMETER target='model.encoder.norm.bias'
p_model_tgt_embed_weight: PARAMETER target='model.tgt_embed.weight'
p_model_decoder_layers_0_self_attn_in_proj_weight: PARAMETER target='model.decoder.layers.0.self_attn.in_proj_weight'
p_model_decoder_layers_0_self_attn_in_proj_bias: PARAMETER target='model.decoder.layers.0.self_attn.in_proj_bias'
p_model_decoder_layers_0_self_attn_out_proj_weight: PARAMETER target='model.decoder.layers.0.self_attn.out_proj.weight'
p_model_decoder_layers_0_self_attn_out_proj_bias: PARAMETER target='model.decoder.layers.0.self_attn.out_proj.bias'
p_model_decoder_layers_0_multihead_attn_in_proj_weight: PARAMETER target='model.decoder.layers.0.multihead_attn.in_proj_weight'
p_model_decoder_layers_0_multihead_attn_in_proj_bias: PARAMETER target='model.decoder.layers.0.multihead_attn.in_proj_bias'
p_model_decoder_layers_0_multihead_attn_out_proj_weight: PARAMETER target='model.decoder.layers.0.multihead_attn.out_proj.weight'
p_model_decoder_layers_0_multihead_attn_out_proj_bias: PARAMETER target='model.decoder.layers.0.multihead_attn.out_proj.bias'
p_model_decoder_layers_0_linear1_weight: PARAMETER target='model.decoder.layers.0.linear1.weight'
p_model_decoder_layers_0_linear1_bias: PARAMETER target='model.decoder.layers.0.linear1.bias'
p_model_decoder_layers_0_linear2_weight: PARAMETER target='model.decoder.layers.0.linear2.weight'
p_model_decoder_layers_0_linear2_bias: PARAMETER target='model.decoder.layers.0.linear2.bias'
p_model_decoder_layers_0_norm1_weight: PARAMETER target='model.decoder.layers.0.norm1.weight'
p_model_decoder_layers_0_norm1_bias: PARAMETER target='model.decoder.layers.0.norm1.bias'
p_model_decoder_layers_0_norm2_weight: PARAMETER target='model.decoder.layers.0.norm2.weight'
p_model_decoder_layers_0_norm2_bias: PARAMETER target='model.decoder.layers.0.norm2.bias'
p_model_decoder_layers_0_norm3_weight: PARAMETER target='model.decoder.layers.0.norm3.weight'
p_model_decoder_layers_0_norm3_bias: PARAMETER target='model.decoder.layers.0.norm3.bias'
p_model_decoder_layers_1_self_attn_in_proj_weight: PARAMETER target='model.decoder.layers.1.self_attn.in_proj_weight'
p_model_decoder_layers_1_self_attn_in_proj_bias: PARAMETER target='model.decoder.layers.1.self_attn.in_proj_bias'
p_model_decoder_layers_1_self_attn_out_proj_weight: PARAMETER target='model.decoder.layers.1.self_attn.out_proj.weight'
p_model_decoder_layers_1_self_attn_out_proj_bias: PARAMETER target='model.decoder.layers.1.self_attn.out_proj.bias'
p_model_decoder_layers_1_multihead_attn_in_proj_weight: PARAMETER target='model.decoder.layers.1.multihead_attn.in_proj_weight'
p_model_decoder_layers_1_multihead_attn_in_proj_bias: PARAMETER target='model.decoder.layers.1.multihead_attn.in_proj_bias'
p_model_decoder_layers_1_multihead_attn_out_proj_weight: PARAMETER target='model.decoder.layers.1.multihead_attn.out_proj.weight'
p_model_decoder_layers_1_multihead_attn_out_proj_bias: PARAMETER target='model.decoder.layers.1.multihead_attn.out_proj.bias'
p_model_decoder_layers_1_linear1_weight: PARAMETER target='model.decoder.layers.1.linear1.weight'
p_model_decoder_layers_1_linear1_bias: PARAMETER target='model.decoder.layers.1.linear1.bias'
p_model_decoder_layers_1_linear2_weight: PARAMETER target='model.decoder.layers.1.linear2.weight'
p_model_decoder_layers_1_linear2_bias: PARAMETER target='model.decoder.layers.1.linear2.bias'
p_model_decoder_layers_1_norm1_weight: PARAMETER target='model.decoder.layers.1.norm1.weight'
p_model_decoder_layers_1_norm1_bias: PARAMETER target='model.decoder.layers.1.norm1.bias'
p_model_decoder_layers_1_norm2_weight: PARAMETER target='model.decoder.layers.1.norm2.weight'
p_model_decoder_layers_1_norm2_bias: PARAMETER target='model.decoder.layers.1.norm2.bias'
p_model_decoder_layers_1_norm3_weight: PARAMETER target='model.decoder.layers.1.norm3.weight'
p_model_decoder_layers_1_norm3_bias: PARAMETER target='model.decoder.layers.1.norm3.bias'
p_model_generator_weight: PARAMETER target='model.generator.weight'
p_model_generator_bias: PARAMETER target='model.generator.bias'
p_model_mask_decoder_0_weight: PARAMETER target='model.mask_decoder.0.weight'
p_model_mask_decoder_0_bias: PARAMETER target='model.mask_decoder.0.bias'
p_model_mask_decoder_1_weight: PARAMETER target='model.mask_decoder.1.weight'
p_model_mask_decoder_1_bias: PARAMETER target='model.mask_decoder.1.bias'
p_model_mask_decoder_3_weight: PARAMETER target='model.mask_decoder.3.weight'
p_model_mask_decoder_3_bias: PARAMETER target='model.mask_decoder.3.bias'
b_model_encoder_pos_encoder_pe: BUFFER target='model.encoder.pos_encoder.pe' persistent=True
b_model_pos_encoder_pe: BUFFER target='model.pos_encoder.pe' persistent=True
x: USER_INPUT

# outputs
div: USER_OUTPUT
Ç8
2pkg.torch.export.ExportedProgram.range_constraints{}B
 