# Real-Time Sign Language Translation Deployment Guide

This document explains how the GhSL Sign2Text translation system works, how the exported artifacts fit together, and the exact steps to serve the model on a hosted platform such as Render. It also walks through the integration flow for a web front-end consuming the inference API.

---

## 1. Model System Overview

### 1.1 Architecture Summary

The system implements a **Sequence-to-Sequence (Seq2Seq) translation model** that converts pose landmark sequences directly into text:

```
Visual Input (B, T, 540) → FrameProjector → TemporalTransformerEncoder → TransformerDecoder → Text Output
```

**Key Components:**

- **FrameProjector**: Projects 540-dim landmarks to 160-dim
- **TemporalTransformerEncoder**: 4-layer Transformer encoder (256-dim, 4 heads)
- **TransformerDecoder**: 2-layer auto-regressive decoder for text generation
- **Beam Search**: Decoding with length penalty for fluent outputs

### 1.2 Pre-processing Pipeline

- **Input**: MediaPipe Holistic landmarks (pose, both hands, down-sampled face) per video frame
- **Feature dimension**: 540 floats per frame (33 pose × 4 + 21 hands × 2 × 3 + face subset)
- **Motion gating**: Clips with average motion energy below `motion_floor` are discarded to avoid static inputs
- **Normalization**:
  - Torso-centering and scale normalization at the frame level
  - Global feature mean and std (exported as `global_stats.npz`) applied to every sequence
- **Temporal handling**: Sequences are cropped/padded to 64 frames

### 1.3 Visual Encoder Architecture

#### FrameProjector

```
LayerNorm → Linear(540, 160) → GELU → Dropout(0.1) → Linear(160, 160) → GELU
```

Projects per-frame 540-dim pose features to a compact 160-dim representation.

#### TemporalTransformerEncoder

```python
TransformerEncoderLayer(
    d_model=256,
    nhead=4,
    dim_feedforward=512,
    dropout=0.1,
    activation='gelu',
    batch_first=True,
    norm_first=True  # Pre-norm architecture
)
# 4 layers total
```

**Features:**

- Sinusoidal positional encoding for temporal awareness
- Pre-norm architecture for training stability
- Mean pooling over time dimension for contrastive embeddings
- L2 normalization of output embeddings
- Returns both pooled embedding (256-d) and full sequence context for decoder

### 1.4 Text Decoder Architecture

Auto-regressive Transformer decoder for text generation:

```python
TransformerDecoderLayer(
    d_model=256,
    nhead=4,
    dim_feedforward=1024,
    dropout=0.1,
    activation='gelu'
)
# 2 layers total
```

**Components:**

- `tgt_embed`: Token embedding layer (vocab_size → 256)
- `pos_encoder`: Sinusoidal positional encoding
- `generator`: Output projection (256 → vocab_size)

### 1.5 Inference Modes

The system supports two inference modes:

#### A. Translation Mode (Primary)

Direct text generation via beam search:

```python
def beam_search_decode(model, visual_input, tokenizer, beam_size=4, length_penalty=0.8):
    # Encode visual sequence
    proj = model.projector(visual_input)
    _, _, context = model.encoder(proj, return_sequence=True)
    memory = context.permute(1, 0, 2)  # (T, B, E)
    
    # Beam search decoding
    return _beam_search_single(model, memory, tokenizer, max_len=64, ...)
```

#### B. Retrieval Mode (Fallback)

Embedding-based nearest-neighbor retrieval:

1. Extract visual embedding via `model.encode_visual(x)`
2. Compute cosine similarity against reference embeddings
3. Optional DTW re-ranking and prototype agreement checks

### 1.6 Inference Assets

Generated by `training.ipynb`:

| File | Description |
|------|-------------|
| `sign_encoder.onnx` | ONNX model (opset 18), dynamic batch/frame axes |
| `embeddings.npy` | Reference embeddings for retrieval fallback |
| `embeddings_map.csv` | Metadata mapping (video_file, sentence_id, sentence, category) |
| `prototypes.npz` | Mean embeddings per sentence class |
| `global_stats.npz` | Feature normalization statistics (mean, std) |
| `model_config.json` | Training configuration snapshot |
| `export_manifest.json` | Integrity checksums and export metadata |

---

## 2. Beam Search Decoding

### 2.1 Algorithm Overview

The decoder uses beam search with length penalty for balanced output generation:

```python
def _beam_search_single(model, memory, tokenizer, device, max_len, beam_size, length_penalty):
    bos = tokenizer.sos_token_id
    eos = tokenizer.eos_token_id
    beams = [(0.0, [bos], False)]  # (log_prob, sequence, is_done)
    finished = []

    for step in range(max_len):
        new_beams = []
        for logprob, seq, done in beams:
            if done:
                new_beams.append((logprob, seq, True))
                continue
            
            # Decode next token
            tgt = torch.tensor(seq, device=device).unsqueeze(0)
            tgt_mask = generate_square_subsequent_mask(tgt.size(1)).to(device)
            tgt_emb = model.tgt_embed(tgt).permute(1, 0, 2)
            tgt_emb = model.pos_encoder(tgt_emb)
            decoder_out = model.decoder(tgt_emb, memory, tgt_mask=tgt_mask)
            logits = model.generator(decoder_out[-1].unsqueeze(0))
            log_probs = torch.log_softmax(logits.squeeze(0), dim=-1)
            
            # Expand beam
            topk = torch.topk(log_probs, k=beam_size)
            for value, index in zip(topk.values.tolist(), topk.indices.tolist()):
                next_seq = seq + [index]
                next_done = index == eos
                next_logprob = logprob + value
                new_beams.append((next_logprob, next_seq, next_done))
                if next_done:
                    finished.append((next_logprob, next_seq))
        
        # Prune to top-k beams
        new_beams.sort(key=lambda x: x[0], reverse=True)
        beams = new_beams[:beam_size]
        
        if all(done for _, _, done in beams):
            break
    
    # Select best with length penalty
    return _select_best(finished or beams, length_penalty)
```

### 2.2 Length Penalty

Prevents overly short predictions:

$$score_{adjusted} = \frac{score}{((5 + length) / 6)^\alpha}$$

Where α = 0.8 (configurable via `beam_length_penalty`)

### 2.3 Configuration

```python
CFG = {
    'beam_size': 4,
    'beam_length_penalty': 0.8,
    'decoder_max_len': 64,
}
```

---

## 3. Streaming Inference

### 3.1 Motion-Gated Inference Flow

For real-time webcam applications:

```python
class StreamingDecoder:
    def __init__(self, model, tokenizer, cfg):
        self.model = model.eval()
        self.tokenizer = tokenizer
        self.buffer = []
        self.min_frames = cfg.get('min_frames', 16)
        self.max_frames = cfg['seq_len']
        self.motion_threshold = cfg['motion_floor']
    
    def process_frame(self, frame_landmarks):
        """Process incoming frame and optionally trigger inference."""
        self.buffer.append(frame_landmarks)
        
        if len(self.buffer) < self.min_frames:
            return None
        
        # Check motion energy
        motion = self._compute_motion_energy()
        if motion < self.motion_threshold:
            return {'status': 'waiting', 'reason': 'low_motion'}
        
        # Trigger inference
        if len(self.buffer) >= self.max_frames:
            return self._decode_buffer()
        
        return {'status': 'buffering', 'frames': len(self.buffer)}
    
    def _decode_buffer(self):
        """Run beam search on buffer and clear."""
        seq = self._preprocess_buffer()
        with torch.no_grad():
            tokens = beam_search_decode(
                self.model, seq, self.tokenizer,
                beam_size=4, length_penalty=0.8
            )
        text = self.tokenizer.decode(tokens, skip_special_tokens=True)
        self.buffer.clear()
        return {'status': 'decoded', 'text': text}
```

### 3.2 Visualization Support

The streaming decoder includes optional gate energy visualization:

```python
def _compute_motion_energy(self):
    """Compute per-frame motion energy for visualization."""
    if len(self.buffer) < 2:
        return 0.0
    frames = np.array(self.buffer)
    motion = np.linalg.norm(frames[1:] - frames[:-1], axis=-1)
    return float(motion.mean())
```

---

## 4. Backend Service Architecture

### 4.1 Configuration (`backend/app/config.py`)

Environment variables with sensible defaults:

| Variable | Purpose | Default |
|----------|---------|---------|
| `MODELS_DIR` | Directory for inference artifacts | `backend/models` |
| `ONNX_MODEL_PATH` | Path to encoder ONNX | `${MODELS_DIR}/sign_encoder.onnx` |
| `EMBEDDINGS_PATH` | Retrieval embeddings | `${MODELS_DIR}/embeddings.npy` |
| `MAPPING_PATH` | Metadata CSV | `${MODELS_DIR}/embeddings_map.csv` |
| `PROTOTYPES_PATH` | Optional prototype cache | `${MODELS_DIR}/prototypes.npz` |
| `GLOBAL_STATS_PATH` | Feature normalization stats | `${MODELS_DIR}/global_stats.npz` |
| `SEQ_LEN` | Nominal clip length | `64` |
| `FEATURE_DIM` | Feature dimension | `540` |
| `DEFAULT_TOP_K` | Default retrieval fan-out | `5` |
| `MOTION_FLOOR` | Motion gating threshold | `7.5e-4` |
| `SIM_THRESHOLD` | Acceptance similarity threshold | `0.58` |
| `SIM_MARGIN` | Minimum gap between top-1 and top-2 | `0.10` |
| `CONFIDENCE_TEMPERATURE` | Softmax temperature | `0.12` |
| `BEAM_SIZE` | Beam search width | `4` |
| `BEAM_LENGTH_PENALTY` | Length penalty alpha | `0.8` |
| `ENABLE_CUDA` | Allow GPU providers | `true` |

### 4.2 Model Architecture (`backend/app/services/inference_model.py`)

The backend uses the same Transformer architecture as training:

```python
class SignTranslationModel(nn.Module):
    def __init__(
        self,
        input_dim=540,
        vocab_size=1000,
        proj_dim=160,
        embed_dim=256,
        attn_heads=4,
        encoder_layers=4,
        encoder_ff_dim=512,
        encoder_dropout=0.1
    ):
        self.projector = FrameProjector(input_dim, proj_dim)
        self.encoder = TemporalTransformerEncoder(
            proj_dim, embed_dim, encoder_layers, attn_heads, encoder_ff_dim, encoder_dropout
        )
        self.tgt_embed = nn.Embedding(vocab_size, embed_dim)
        self.pos_encoder = PositionalEncoding(embed_dim)
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=2)
        self.generator = nn.Linear(embed_dim, vocab_size)
```

### 4.3 Inference Service (`backend/app/services/inference_seq2seq.py`)

```python
class Seq2SeqInferenceService:
    def __init__(self):
        self.model = self._load_model()
        self.tokenizer = self._load_tokenizer()
        self.global_mean, self.global_std = self._load_stats()
    
    def translate(self, frames: np.ndarray) -> dict:
        """Translate pose sequence to text."""
        # Preprocess
        seq = self._preprocess(frames)
        
        # Motion gate
        motion = compute_motion_energy(frames)
        if motion.mean() < CFG['motion_floor']:
            return {'accepted': False, 'reason': 'low_motion'}
        
        # Beam search decode
        with torch.no_grad():
            tokens = beam_search_decode(
                self.model, seq, self.tokenizer,
                beam_size=CFG['beam_size'],
                length_penalty=CFG['beam_length_penalty']
            )
        
        text = self.tokenizer.decode(tokens, skip_special_tokens=True)
        return {
            'accepted': True,
            'translation': text,
            'motion_mean': float(motion.mean())
        }
```

### 4.4 API Surface (`backend/app/main.py`)

| Endpoint | Description |
|----------|-------------|
| `GET /healthz` | Health check; returns model and runtime info |
| `GET /v1/config` | Exposes model configuration |
| `GET /v1/samples/random` | Random reference sample for UI examples |
| `POST /v1/inference` | Main inference endpoint |
| `POST /v1/translate` | Direct translation endpoint (Seq2Seq) |

#### Translation Request (`/v1/translate`)

```json
{
  "frames": [[...540 floats...], ...],
  "metadata": {
    "fps": 30,
    "source": "webcam"
  }
}
```

#### Translation Response

```json
{
  "accepted": true,
  "translation": "hello how are you",
  "motion_mean": 0.0013,
  "confidence": 0.85,
  "beam_alternatives": [
    {"text": "hello how are you", "score": -2.3},
    {"text": "hi how are you", "score": -2.8}
  ]
}
```

#### Retrieval Request (`/v1/inference`)

```json
{
  "frames": [[...540 floats...], ...],
  "top_k": 5,
  "metadata": {
    "fps": 30,
    "source": "webcam"
  }
}
```

#### Retrieval Response

```json
{
  "top_k": 5,
  "results": [
    {
      "sentence_id": 42,
      "sentence": "THANK YOU",
      "category": "greeting",
      "similarity": 0.83,
      "confidence": 0.91,
      "accepted": true
    }
  ]
}
```

---

## 5. Deploying on Render

### 5.1 Prerequisites

- Render account with Web Services access
- Container-ready repository with `backend/` folder
- Model artifacts committed or in persistent storage
- Python ≥ 3.10, PyTorch ≥ 2.1 (ONNX Runtime for production)

### 5.2 Repository Layout

```
backend/
  ├─ app/
  │   ├─ __init__.py
  │   ├─ main.py
  │   ├─ config.py
  │   ├─ dependencies.py
  │   ├─ schemas.py
  │   └─ services/
  │       ├─ inference_model.py
  │       └─ inference_seq2seq.py
  ├─ models/
  │   ├─ sign_encoder.onnx
  │   ├─ embeddings.npy
  │   ├─ embeddings_map.csv
  │   ├─ prototypes.npz
  │   ├─ global_stats.npz
  │   └─ model_config.json
  └─ requirements.txt
```

### 5.3 Render Configuration

1. **Create Web Service** from repository
2. **Environment**:
   - Runtime: Python 3.x
   - Build: `pip install -r backend/requirements.txt`
   - Start: `uvicorn backend.app.main:app --host 0.0.0.0 --port $PORT`

3. **Environment Variables**:

```yaml
MODELS_DIR: /opt/render/project/src/backend/models
GLOBAL_STATS_PATH: /opt/render/project/src/backend/models/global_stats.npz
MOTION_FLOOR: 0.00075
SIM_THRESHOLD: 0.58
BEAM_SIZE: 4
BEAM_LENGTH_PENALTY: 0.8
ENABLE_CUDA: false
```

4. Deploy and verify `/healthz`

---

## 6. Front-end Integration

### 6.1 Real-Time Flow

```
┌─────────────┐     ┌──────────────┐     ┌─────────────┐     ┌──────────┐
│   Webcam    │────▶│  MediaPipe   │────▶│   Buffer    │────▶│  Backend │
│   Stream    │     │  Holistic    │     │  Manager    │     │  /v1/    │
└─────────────┘     └──────────────┘     └─────────────┘     └──────────┘
                                                                   │
                                                                   ▼
┌─────────────┐     ┌──────────────┐     ┌─────────────┐     ┌──────────┐
│    UI       │◀────│   Display    │◀────│   Parse     │◀────│ Response │
│   Update    │     │  Translation │     │   JSON      │     │   JSON   │
└─────────────┘     └──────────────┘     └─────────────┘     └──────────┘
```

### 6.2 Client-Side Pseudocode

```javascript
// Buffer management
const frameBuffer = [];
const MIN_FRAMES = 16;
const MAX_FRAMES = 64;
const MOTION_THRESHOLD = 0.001;

function onMediaPipeResults(results) {
  const landmarks = extractLandmarks(results);  // → float[540]
  frameBuffer.push(landmarks);
  
  if (frameBuffer.length < MIN_FRAMES) return;
  
  // Check motion
  const motion = computeMotion(frameBuffer);
  if (motion < MOTION_THRESHOLD) {
    updateUI({ status: 'waiting', message: 'Please sign...' });
    return;
  }
  
  // Send for translation
  if (frameBuffer.length >= MAX_FRAMES || shouldTrigger()) {
    translateBuffer();
  }
}

async function translateBuffer() {
  const payload = {
    frames: frameBuffer.map(f => Array.from(f)),
    metadata: { fps: 30, source: 'webcam' }
  };
  
  const response = await fetch('/v1/translate', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify(payload)
  });
  
  const data = await response.json();
  
  if (data.accepted) {
    displayTranslation(data.translation);
  } else {
    showRejectionReason(data.reason);
  }
  
  frameBuffer.length = 0;  // Clear buffer
}
```

### 6.3 Landmark Overlay Visualization

The frontend supports optional landmark skeleton overlay:

```typescript
function drawLandmarks(ctx: CanvasRenderingContext2D, results: Results) {
  // Draw pose connections
  if (results.poseLandmarks) {
    drawConnectors(ctx, results.poseLandmarks, POSE_CONNECTIONS, { color: '#00FF00' });
    drawLandmarks(ctx, results.poseLandmarks, { color: '#FF0000', radius: 2 });
  }
  
  // Draw hand connections
  if (results.leftHandLandmarks) {
    drawConnectors(ctx, results.leftHandLandmarks, HAND_CONNECTIONS, { color: '#CC0000' });
  }
  if (results.rightHandLandmarks) {
    drawConnectors(ctx, results.rightHandLandmarks, HAND_CONNECTIONS, { color: '#00CC00' });
  }
}
```

---

## 7. Evaluation Metrics

### 7.1 Translation Quality

| Metric | Description | Target |
|--------|-------------|--------|
| **WER** | Word Error Rate (lower is better) | < 30% |
| **BLEU-4** | N-gram precision (higher is better) | > 20.0 |
| **Exact Match** | Perfect predictions | > 10% |

### 7.2 Retrieval Quality (Fallback)

| Metric | Description | Target |
|--------|-------------|--------|
| **Ret@1** | Top-1 retrieval accuracy | > 65% |
| **MRR** | Mean Reciprocal Rank | > 75% |
| **PosSim** | Positive pair similarity | > 0.85 |
| **NegSim** | Negative pair similarity | < 0.50 |

### 7.3 Runtime Performance

| Metric | Target |
|--------|--------|
| Encoding latency | < 50ms |
| Beam search latency | < 100ms |
| Total inference | < 200ms |

---

## 8. Quick Checklist

- [ ] Export notebook artifacts (training.ipynb)
- [ ] Copy artifacts to `backend/models/`
- [ ] Verify model architecture matches (Transformer encoder, not GRU)
- [ ] Set environment variables on Render
- [ ] Deploy and test `/healthz`
- [ ] Test `/v1/translate` with sample sequence
- [ ] Integrate frontend MediaPipe + buffer management
- [ ] Monitor translation quality metrics
- [ ] Adjust beam size / length penalty as needed

---

## 9. Troubleshooting

### Common Issues

1. **RuntimeError: state_dict mismatch**
   - Cause: Model architecture mismatch (e.g., GRU vs Transformer)
   - Fix: Ensure `inference_model.py` uses `TemporalTransformerEncoder`

2. **Low translation quality**
   - Check: Motion gating threshold
   - Check: Input normalization (global_stats.npz)
   - Try: Increase beam size, adjust length penalty

3. **High latency**
   - Try: Reduce beam size (4 → 2)
   - Try: Enable CUDA if GPU available
   - Check: ONNX Runtime optimization level

4. **Empty translations**
   - Check: Tokenizer vocabulary matches training
   - Check: SOS/EOS token IDs are correct
   - Verify: `decoder_max_len` is sufficient

