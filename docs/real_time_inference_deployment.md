# Real-Time Sign Retrieval Deployment Guide

This document explains how the refreshed GhSL encoder works, how the exported artifacts fit together, and the exact steps to serve the model on a hosted platform such as Render. It also walks through the integration flow for a web front-end consuming the inference API.

---

## 1. Model System Overview

### 1.1 Pre-processing Pipeline
- **Input**: MediaPipe Holistic landmarks (pose, both hands, down-sampled face) per video frame.
- **Motion gating**: Clips with average motion energy below `motion_floor` are discarded to avoid static inputs.
- **Normalization**:
  - Torso-centering and scale normalization at the frame level (see `real_time_inference.py`).
  - Global feature mean and std (exported as `proc/global_stats.npz`) are applied to every sequence.
- **Temporal handling**: Sequences are cropped/padded to 64 frames, with light stochastic augmentations during training and center crops during evaluation.
- **Outputs**: `processed_metadata.csv`, normalized `.npy` feature files, `global_stats.npz`, and optional motion statistics.

### 1.2 Encoder Architecture
- **FrameProjector**: `LayerNorm → Linear → GELU → Dropout → Linear → GELU`. Projects 540-dim frame features down to 160-dim.
- **TemporalEncoder**: 2-layer bidirectional GRU (160 hidden units) with motion-aware attention heads.
  - Attention scores blend learned weights with normalized motion energy per timestep to focus on highly dynamic frames.
  - Outputs L2-normalized 256-d embeddings.
- **Training Objective**: InfoNCE contrastive loss over paired augmentations of the same clip.
- **Retrieval Metrics**: Validation includes alignment (Inst@1/PosSim/NegSim) and sentence-level retrieval (Ret@1, MRR).

### 1.3 Inference Assets
Generated by `training.ipynb` (Cells 16–20):
- `sign_encoder.onnx`: Dynamic batch and frame length ONNX model (opset 18).
- `sign_encoder.ts`: TorchScript tracer for PyTorch-based runners.
- `embeddings.npy`, `embeddings_map.csv`: Reference database for nearest-neighbour retrieval.
- `prototypes.npz`: Mean embeddings per sentence for prototype gating.
- `model_config.json`: Snapshot of the training `CFG` used when exporting.
- `export_manifest.json`, `sign_encoder_bundle.zip`: Integrity info and consolidated bundle for deployment.

### 1.4 Runtime Decision Logic
1. **Sequence prep**: Center crop long sequences, normalize with global stats, ensure at least 8 frames via repeat padding.
2. **Motion gate**: Reject clips with low motion early.
3. **Embedding**: Feed preprocessed sequence into ONNX model, getting a normalized sentence embedding.
4. **Similarity search**: Compute cosine similarity against cached reference embeddings.
5. **Optional DTW re-ranking**: If `fastdtw` is available and enabled, re-rank top candidates with projected sequence-level DTW distance.
6. **Prototype agreement**: Compare top result with class prototype; enforce similarity margin and prototype agreement before accepting.
7. **Return metadata**: Include raw vs refined similarities, DTW diagnostics, prototype score, motion statistics, and acceptance status.

---

## 2. Packaging for Deployment
1. Run `training.ipynb` through Cells 16–20 to regenerate fresh artifacts after any training updates.
2. Ensure the following files exist in `runs/` (or desired export directory):
   - `sign_encoder.onnx`
   - `sign_encoder.ts`
   - `embeddings.npy`
   - `embeddings_map.csv`
   - `prototypes.npz` (optional but recommended)
   - `global_stats.npz` (from `proc/`)
   - `model_config.json`
   - `export_manifest.json`
   - `sign_encoder_bundle.zip`
3. Copy the required files into `backend/models/` (or the directory you mount inside your container). Suggested structure:
   ```text
   backend/models/
     ├─ sign_encoder.onnx
     ├─ embeddings.npy
     ├─ embeddings_map.csv
     ├─ prototypes.npz
     ├─ global_stats.npz
     └─ model_config.json
   ```
4. Keep `export_manifest.json` and the zip archive for auditing but they are not required by the server.

---

## 3. Backend Service Anatomy

### 3.1 Configuration (`backend/app/config.py`)
Environment variables with sensible defaults:
| Variable | Purpose | Default |
| --- | --- | --- |
| `MODELS_DIR` | Directory for inference artifacts | `backend/models` |
| `ONNX_MODEL_PATH` | Path to encoder ONNX | `${MODELS_DIR}/sign_encoder.onnx` |
| `EMBEDDINGS_PATH` | Retrieval embeddings | `${MODELS_DIR}/embeddings.npy` |
| `MAPPING_PATH` | Metadata CSV | `${MODELS_DIR}/embeddings_map.csv` |
| `PROTOTYPES_PATH` | Optional prototype cache | `${MODELS_DIR}/prototypes.npz` |
| `GLOBAL_STATS_PATH` | Feature normalization stats | `proc/global_stats.npz` |
| `SEQ_LEN` | Nominal clip length | `64` |
| `FEATURE_DIM` | Feature dimension | `540` |
| `DEFAULT_TOP_K` | Default retrieval fan-out | `5` |
| `MOTION_FLOOR` | Motion gating threshold | `7.5e-4` |
| `SIM_THRESHOLD` | Acceptance similarity threshold | `0.58` |
| `SIM_MARGIN` | Minimum gap between top-1 and top-2 | `0.10` |
| `CONFIDENCE_TEMPERATURE` | Softmax temperature for per-candidate confidence | `0.12` |
| `DTW_ENABLED` | Enable DTW re-ranking | `true` |
| `DTW_RADIUS` | DTW search radius | `6` |
| `DTW_ALPHA` | Blend weight for cosine similarity vs DTW score | `0.65` |
| `DTW_LAMBDA` | Exponential decay for DTW distance | `0.002` |
| `DTW_TOPK` | Number of candidates to re-rank | `5` |
| `ENABLE_CUDA` | Allow GPU providers in ONNX Runtime | `true` |
| `VIDEOS_DIR` | Reference clips for `/videos/{filename}` | `sample_dataset/videos` |

### 3.2 Inference Service (`backend/app/services/inference.py`)
- Loads ONNX model with dynamic axes, global stats, embedding database, optional prototypes.
- Enforces motion gating, normalizes sequences, handles repeat padding for short clips.
- Executes ONNX inference inside a thread-safe guard.
- Performs optional DTW re-ranking if `fastdtw` is installed and `DTW_ENABLED=true`.
- Returns `Prediction` objects with enriched metadata (raw similarity, prototype score, DTW diagnostics, acceptance flag).

### 3.3 API Surface (`backend/app/main.py`)
| Endpoint | Description |
| --- | --- |
| `GET /healthz` | Basic health check; returns ONNX Runtime provider info. |
| `GET /v1/config` | Exposes model configuration (sequence length, thresholds, DTW status). |
| `GET /v1/samples/random` | Provides random reference metadata for UI examples. |
| `GET /videos/{filename}` | Streams a reference video from `VIDEOS_DIR`. |
| `POST /v1/inference` | Main inference endpoint (see flow below). |

Request body for `/v1/inference` (`SequencePayload`):
```json
{
  "frames": [[...540 floats...], ...],
  "top_k": 5,
  "metadata": {
    "fps": 30,
    "source": "webcam"
  }
}
```
Response (`InferenceResponse`):
```json
{
  "top_k": 3,
  "results": [
    {
      "sentence_id": 42,
      "sentence": "THANK YOU",
      "category": "greeting",
      "video_file": "thank_you.mp4",
      "similarity": 0.83,
      "confidence": 0.91,
      "raw_similarity": 0.80,
      "proto_similarity": 0.78,
      "motion_mean": 0.0013,
      "accepted": true,
      "rejection_reason": null,
      "dtw_distance": 12.4,
      "dtw_score": 0.94
    }
  ]
}
```
`accepted=false` plus `rejection_reason` indicates the top match failed a gating rule (low similarity, ambiguous top-2, prototype disagreement).

---

## 4. Deploying on Render

### 4.1 Prerequisites
- Render account with access to Web Services.
- Container-ready repository including `backend/` folder and `requirements.txt`.
- Model artifacts committed or uploaded to persistent storage (Render Disk, S3, etc.).
- Python runtime ≥ 3.10, PyTorch ≥ 2.1 for development; the deployed API relies only on ONNX Runtime.

### 4.2 Suggested Repository Layout for Deployment
```
backend/
  ├─ app/
  ├─ models/
  │   ├─ sign_encoder.onnx
  │   ├─ embeddings.npy
  │   ├─ embeddings_map.csv
  │   ├─ prototypes.npz
  │   ├─ global_stats.npz
  │   └─ model_config.json
  ├─ requirements.txt
  └─ run.sh  (optional convenience script)
```

### 4.3 Render Web Service Configuration
1. **Create Web Service** from your repository.
2. **Environment**:
   - Runtime: `Python 3.x`.
   - Build command: `pip install -r backend/requirements.txt`.
   - Start command: `uvicorn backend.app.main:app --host 0.0.0.0 --port $PORT`.
   - If using GPU instances, ensure `onnxruntime-gpu` is in requirements (adjust accordingly).
3. **Environment Variables** (set via Render dashboard or `render.yaml`):
   - `MODELS_DIR=/opt/render/project/src/backend/models`
   - `GLOBAL_STATS_PATH=/opt/render/project/src/backend/models/global_stats.npz`
   - `PROTOTYPES_PATH=/opt/render/project/src/backend/models/prototypes.npz`
   - `DEFAULT_TOP_K=5`
   - `MOTION_FLOOR=0.00075`
   - `SIM_THRESHOLD=0.58`
   - `SIM_MARGIN=0.1`
   - `CONFIDENCE_TEMPERATURE=0.12`
   - `DTW_ENABLED=true`
   - `DTW_RADIUS=6`
   - `DTW_ALPHA=0.65`
   - `DTW_LAMBDA=0.002`
   - `DTW_TOPK=5`
   - `ENABLE_CUDA=false` (unless using GPU plan)
4. **Persistent Storage** (optional): attach a Render Disk if you expect to refresh embeddings without redeploying.
5. Deploy and verify `/healthz`.
6. Run a smoke test against `/v1/inference` with a known clip to confirm the pipeline (see Section 5).

---

## 5. Front-end Integration Flow

### 5.1 Typical Real-Time Flow
1. **Capture**: Front-end loops on webcam frames (WebRTC or MediaDevices API).
2. **Landmark extraction**:
   - Option A: Run MediaPipe or TensorFlow.js model in the browser, outputting landmarks per frame.
   - Option B: Stream raw video frames to the backend for feature extraction (more bandwidth).
   - Recommended: Perform holistic landmark extraction client-side to minimize latency and payload size.
3. **Buffer management**: Maintain a sliding window (e.g., 64–96 frames). Once `min_frames` reached, dispatch inference.
4. **Packaging**: Convert each frame to a `float32[540]` list and send to backend via `fetch` POST to `/v1/inference`.
5. **Response handling**: Display ranked predictions, highlight acceptance status, show fallback guidance if rejected.
6. **Cooldown**: Optionally limit requests (e.g., send every 1.5 seconds) to avoid redundant queries.

### 5.2 Example Front-end Pseudocode
```javascript
async function runInference(buffer) {
  const payload = {
    frames: buffer.map(frame => Array.from(frame)),
    top_k: 5,
    metadata: { fps: currentFps, source: 'webcam' }
  };

  const response = await fetch('https://<render-service>/v1/inference', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify(payload)
  });

  if (!response.ok) {
    const err = await response.json();
    throw new Error(err.detail || 'Inference failed');
  }

  const data = await response.json();
  // data.results[0].accepted indicates whether the top prediction passed gating
  updateUIWithPredictions(data.results);
}
```

### 5.3 Example Interaction Sequence
1. **Client** collects 70 frames (`buffer.length >= 32`) → preprocess (landmarks → normalized arrays).
2. **Client** sends payload to `/v1/inference`.
3. **Server** computes embedding, enforces gating, returns predictions with metadata.
4. **Client** updates UI with sentence text, similarity/confidence, gating status.
5. **Optional**: Client fetches `/v1/config` on startup to adapt thresholds or top-k choices.

---

## 6. Testing & Maintenance

1. **Local smoke test**: Run `uvicorn backend.app.main:app --reload` locally; send sample sequences captured from the training dataset.
2. **Regression checks**: After retraining, run the notebook export cells and diff `export_manifest.json` to ensure artifacts changed as expected.
3. **Performance monitoring**: Track latency on Render; consider enabling ONNX Runtime profiling if needed (`ORT_PROFILER=1`).
4. **Fallback path**: If ONNX Runtime fails to load (e.g., due to opset mismatch), deploy using TorchScript artifacts with a PyTorch runner.
5. **Updating embeddings**: Re-run inference export, replace files in `backend/models/`, redeploy. Keep `model_config.json` in sync.

---

## 7. Quick Checklist
- [ ] Export notebook artifacts (Cells 16–20).
- [ ] Copy artifacts into `backend/models/`.
- [ ] Set environment variables on Render (or target platform).
- [ ] Install requirements and deploy backend API.
- [ ] Verify `/healthz` and `/v1/inference` locally or via staging.
- [ ] Integrate front-end streaming landmarks and posting to the inference endpoint.
- [ ] Monitor gating metrics and adjust thresholds as needed.

With these steps, you can serve the GhSL sentence retrieval encoder in a real-time web application, leveraging the same logic as the local demo while scaling via an ONNX Runtime-backed API.

