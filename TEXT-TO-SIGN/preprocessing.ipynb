{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a887f6",
   "metadata": {},
   "source": [
    "## Cell 1 ‚Äî Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ad4d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "from scipy import signal\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf09f5",
   "metadata": {},
   "source": [
    "## Cell 2 ‚Äî Paths, Feature Shapes, Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"./text2sign_dataset\"  # update if different\n",
    "VIDEO_DIR = os.path.join(DATASET_DIR, \"videos\")\n",
    "META_PATH = os.path.join(DATASET_DIR, \"metadata.csv\")\n",
    "\n",
    "OUTPUT_DIR = \"./features/text2sign_pose\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "GLOBAL_STATS_PATH = \"./proc/text2sign_global_stats.npz\"\n",
    "os.makedirs(os.path.dirname(GLOBAL_STATS_PATH), exist_ok=True)\n",
    "\n",
    "# Feature dimensions (world coords preferred)\n",
    "FACE_DOWNSAMPLE = 5\n",
    "NUM_FACE_LANDMARKS = len(range(0, 468, FACE_DOWNSAMPLE))  # 94\n",
    "NUM_POSE_LANDMARKS = 33\n",
    "NUM_HAND_LANDMARKS = 21\n",
    "\n",
    "POSE_FEATURES = NUM_POSE_LANDMARKS * 4          # x,y,z,visibility\n",
    "HANDS_FEATURES = 2 * NUM_HAND_LANDMARKS * 3      # both hands, xyz\n",
    "FACE_FEATURES = NUM_FACE_LANDMARKS * 3\n",
    "PER_FRAME_FEATURES = POSE_FEATURES + HANDS_FEATURES + FACE_FEATURES\n",
    "\n",
    "# Quality / processing knobs\n",
    "TARGET_SEQ_LEN = 96          # generative: keep longer motion when present\n",
    "TARGET_FPS = 30              # resample to fixed FPS\n",
    "MODEL_COMPLEXITY = 2\n",
    "MIN_FRAMES = 32              # lighter gate vs classification\n",
    "MIN_VALID_RATIO = 0.30       # allow more sparse detections\n",
    "EMA_ALPHA = 0.15             # light temporal smoothing\n",
    "VISIBILITY_THRESHOLD = 0.35  # keep more frames\n",
    "MOTION_KEEP_THRESHOLD = 5e-4 # lenient; we want pauses too\n",
    "MOTION_REJECTION_THRESHOLD = 0.0  # disable hard rejection for generation\n",
    "KEEP_LOW_QUALITY = True      # keep flagged clips, mark in metadata\n",
    "\n",
    "LEFT_SHOULDER = 11; RIGHT_SHOULDER = 12; LEFT_HIP = 23; RIGHT_HIP = 24\n",
    "\n",
    "print(\"üìê Feature dimensions:\")\n",
    "print(f\"  ‚Ä¢ Per-frame: {PER_FRAME_FEATURES}\")\n",
    "print(\"‚öôÔ∏è Quality (lenient for generation):\")\n",
    "print(f\"  ‚Ä¢ min_frames: {MIN_FRAMES}, min_valid_ratio: {MIN_VALID_RATIO}\")\n",
    "print(f\"  ‚Ä¢ visibility min: {VISIBILITY_THRESHOLD}, motion keep ‚â• {MOTION_KEEP_THRESHOLD}\")\n",
    "print(f\"  ‚Ä¢ Target FPS: {TARGET_FPS}, Target seq len: {TARGET_SEQ_LEN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6511ce",
   "metadata": {},
   "source": [
    "## Cell 3 ‚Äî Metadata Loader (robust CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca0975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "records = []\n",
    "with open(META_PATH, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        records.append(row)\n",
    "\n",
    "df_meta = pd.DataFrame(records)\n",
    "df_meta['sentence_id'] = df_meta['sentence_id'].astype(str)\n",
    "\n",
    "print(f\"‚úÖ Loaded metadata: {len(df_meta)} videos\")\n",
    "print(df_meta.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3df78",
   "metadata": {},
   "source": [
    "## Cell 4 ‚Äî Geometry Helpers (Canonical Avatar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "HOLISTIC = None\n",
    "FACE_INDICES = list(range(0, 468, FACE_DOWNSAMPLE))\n",
    "\n",
    "# --- Canonicalization steps ---\n",
    "def canonicalize(points_xyz: np.ndarray):\n",
    "    \"\"\"points_xyz: (N,3) in world or image space. Returns centered + scaled + aligned.\"\"\"\n",
    "    pts = points_xyz.copy()\n",
    "    root = 0.5 * (pts[LEFT_HIP] + pts[RIGHT_HIP])\n",
    "    pts -= root\n",
    "\n",
    "    spine_vec = 0.5 * (pts[LEFT_SHOULDER] + pts[RIGHT_SHOULDER]) - 0.5 * (pts[LEFT_HIP] + pts[RIGHT_HIP])\n",
    "    spine_len = np.linalg.norm(spine_vec)\n",
    "    if spine_len < 1e-4:\n",
    "        spine_len = 1.0\n",
    "    pts /= spine_len\n",
    "\n",
    "    shoulders = pts[RIGHT_SHOULDER] - pts[LEFT_SHOULDER]\n",
    "    yaw = np.arctan2(shoulders[2], shoulders[0] + 1e-8)\n",
    "    rot = np.array([[ np.cos(-yaw), 0, np.sin(-yaw)],\n",
    "                    [ 0,            1, 0           ],\n",
    "                    [-np.sin(-yaw), 0, np.cos(-yaw)]], dtype=np.float32)\n",
    "    pts = pts @ rot.T\n",
    "    return pts.astype(np.float32)\n",
    "\n",
    "\n",
    "def trim_or_pad_sequence(sequence: np.ndarray, target_len: int = TARGET_SEQ_LEN) -> np.ndarray:\n",
    "    frames = sequence.shape[0]\n",
    "    if frames == target_len:\n",
    "        return sequence\n",
    "    if frames > target_len:\n",
    "        start = (frames - target_len) // 2\n",
    "        return sequence[start:start + target_len]\n",
    "    pad_len = target_len - frames\n",
    "    pad = np.zeros((pad_len, sequence.shape[1]), dtype=sequence.dtype)\n",
    "    return np.vstack([sequence, pad])\n",
    "\n",
    "\n",
    "def exponential_smooth(sequence: np.ndarray, alpha: float = EMA_ALPHA) -> np.ndarray:\n",
    "    if alpha <= 0.0 or sequence.shape[0] < 2:\n",
    "        return sequence.astype(np.float32)\n",
    "    out = sequence.astype(np.float32, copy=True)\n",
    "    one_minus = 1.0 - alpha\n",
    "    for i in range(1, out.shape[0]):\n",
    "        out[i] = alpha * out[i] + one_minus * out[i - 1]\n",
    "    return out\n",
    "\n",
    "\n",
    "def resample_frames(frames: np.ndarray, orig_fps: float, target_fps: float = TARGET_FPS):\n",
    "    if orig_fps <= 0 or abs(orig_fps - target_fps) < 1e-3:\n",
    "        return frames\n",
    "    t_orig = np.linspace(0, frames.shape[0] - 1, frames.shape[0])\n",
    "    t_new = np.linspace(0, frames.shape[0] - 1, int(frames.shape[0] * target_fps / max(orig_fps, 1e-6)))\n",
    "    resampled = np.stack([np.interp(t_new, t_orig, frames[:, i]) for i in range(frames.shape[1])], axis=1)\n",
    "    return resampled.astype(np.float32)\n",
    "\n",
    "\n",
    "def compute_motion_energy(frames: np.ndarray) -> np.ndarray:\n",
    "    if frames.shape[0] == 0:\n",
    "        return np.zeros((0,), dtype=np.float32)\n",
    "    diffs = np.diff(frames, axis=0, prepend=frames[:1])\n",
    "    energy = np.linalg.norm(diffs, axis=1)\n",
    "    return energy.astype(np.float32)\n",
    "\n",
    "\n",
    "def summarize_motion(values: np.ndarray) -> dict:\n",
    "    if values.size == 0:\n",
    "        return {\"mean\": 0.0, \"p50\": 0.0, \"p95\": 0.0, \"max\": 0.0}\n",
    "    return {\n",
    "        \"mean\": float(values.mean()),\n",
    "        \"p50\": float(np.median(values)),\n",
    "        \"p95\": float(np.percentile(values, 95)),\n",
    "        \"max\": float(values.max()),\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Helpers ready (canonicalization, smoothing, resampling)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917744c9",
   "metadata": {},
   "source": [
    "## Cell 5 ‚Äî MediaPipe Worker Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b49a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import atexit\n",
    "\n",
    "\n",
    "def worker_init(face_downsample: int = FACE_DOWNSAMPLE):\n",
    "    global HOLISTIC, FACE_INDICES\n",
    "    try:\n",
    "        HOLISTIC = mp_holistic.Holistic(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=MODEL_COMPLEXITY,\n",
    "            enable_segmentation=False,\n",
    "            refine_face_landmarks=False,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5,\n",
    "        )\n",
    "    except Exception:\n",
    "        HOLISTIC = None\n",
    "    FACE_INDICES = list(range(0, 468, max(1, int(face_downsample))))\n",
    "\n",
    "\n",
    "def _close_holistic():\n",
    "    global HOLISTIC\n",
    "    try:\n",
    "        if HOLISTIC is not None:\n",
    "            HOLISTIC.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "atexit.register(_close_holistic)\n",
    "print(\"‚úÖ Worker initializer ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0668913",
   "metadata": {},
   "source": [
    "## Cell 6 ‚Äî Single-Video Processing (world coords + lenient gating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba17215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_video(row_dict, video_dir, output_dir, face_downsample):\n",
    "    import os\n",
    "    video_file = row_dict['video_file']\n",
    "    video_path = os.path.join(video_dir, video_file)\n",
    "    if not os.path.exists(video_path):\n",
    "        return {'status': 'missing', 'video_file': video_file, 'error': 'File not found'}\n",
    "\n",
    "    cap = None\n",
    "    local_holistic = None\n",
    "    holistic_inst = HOLISTIC\n",
    "    try:\n",
    "        if holistic_inst is None:\n",
    "            local_holistic = mp_holistic.Holistic(\n",
    "                static_image_mode=False,\n",
    "                model_complexity=MODEL_COMPLEXITY,\n",
    "                enable_segmentation=False,\n",
    "                refine_face_landmarks=False,\n",
    "                min_detection_confidence=0.5,\n",
    "                min_tracking_confidence=0.5,\n",
    "            )\n",
    "            holistic_inst = local_holistic\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            return {'status': 'failed', 'video_file': video_file, 'error': 'Cannot open video'}\n",
    "        orig_fps = cap.get(cv2.CAP_PROP_FPS) or TARGET_FPS\n",
    "\n",
    "        frames = []\n",
    "        quality_flags = []\n",
    "        vis_scores = []\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            res = holistic_inst.process(rgb)\n",
    "\n",
    "            # Use world landmarks when available; fallback to image landmarks\n",
    "            pose_lm = res.pose_world_landmarks or res.pose_landmarks\n",
    "            if pose_lm is None:\n",
    "                pose_block = [0.0] * (NUM_POSE_LANDMARKS * 4)\n",
    "                pose_vis = 0.0\n",
    "            else:\n",
    "                pose_block = []\n",
    "                for lm in pose_lm.landmark:\n",
    "                    pose_block.extend([lm.x, lm.y, lm.z, getattr(lm, 'visibility', 1.0)])\n",
    "                pose_vis = float(np.mean([getattr(lm, 'visibility', 1.0) for lm in pose_lm.landmark]))\n",
    "\n",
    "            if res.left_hand_landmarks:\n",
    "                lh = [v for lm in res.left_hand_landmarks.landmark for v in (lm.x, lm.y, lm.z)]\n",
    "            else:\n",
    "                lh = [0.0] * (NUM_HAND_LANDMARKS * 3)\n",
    "\n",
    "            if res.right_hand_landmarks:\n",
    "                rh = [v for lm in res.right_hand_landmarks.landmark for v in (lm.x, lm.y, lm.z)]\n",
    "            else:\n",
    "                rh = [0.0] * (NUM_HAND_LANDMARKS * 3)\n",
    "\n",
    "            if res.face_landmarks:\n",
    "                fl = res.face_landmarks.landmark\n",
    "                face_block = []\n",
    "                for i in range(0, 468, face_downsample):\n",
    "                    lm = fl[i]\n",
    "                    face_block.extend([lm.x, lm.y, lm.z])\n",
    "            else:\n",
    "                face_block = [0.0] * (NUM_FACE_LANDMARKS * 3)\n",
    "\n",
    "            frame_vec = pose_block + lh + rh + face_block\n",
    "            if len(frame_vec) < PER_FRAME_FEATURES:\n",
    "                frame_vec.extend([0.0] * (PER_FRAME_FEATURES - len(frame_vec)))\n",
    "            frame_arr = np.asarray(frame_vec[:PER_FRAME_FEATURES], dtype=np.float32)\n",
    "\n",
    "            pose_xyz = frame_arr[:POSE_FEATURES].reshape(NUM_POSE_LANDMARKS, 4)\n",
    "            coords = pose_xyz[:, :3]\n",
    "            coords = canonicalize(coords)\n",
    "            pose_xyz[:, :3] = coords\n",
    "            frame_arr[:POSE_FEATURES] = pose_xyz.reshape(-1)\n",
    "\n",
    "            frames.append(frame_arr)\n",
    "            quality_flags.append(pose_lm is not None or res.left_hand_landmarks or res.right_hand_landmarks)\n",
    "            vis_scores.append(pose_vis)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if not frames:\n",
    "            return {'status': 'failed', 'video_file': video_file, 'error': 'No frames'}\n",
    "\n",
    "        frames = np.stack(frames, axis=0)\n",
    "        motion_vals = compute_motion_energy(frames)\n",
    "        quality_mask = np.array(quality_flags, dtype=bool)\n",
    "        vis_scores = np.array(vis_scores, dtype=np.float32)\n",
    "\n",
    "        valid_idx = np.where(quality_mask)[0]\n",
    "        detection_ratio = float(valid_idx.size / len(frames)) if len(frames) else 0.0\n",
    "        if valid_idx.size == 0:\n",
    "            return {'status': 'failed', 'video_file': video_file, 'error': 'No valid detections'}\n",
    "\n",
    "        frames = frames[valid_idx]\n",
    "        vis_scores = vis_scores[valid_idx]\n",
    "        motion_vals = motion_vals[valid_idx]\n",
    "\n",
    "        keep_mask = (vis_scores >= VISIBILITY_THRESHOLD) | (motion_vals >= MOTION_KEEP_THRESHOLD)\n",
    "        if not keep_mask.any():\n",
    "            keep_mask[0] = True\n",
    "        frames = frames[keep_mask]\n",
    "        vis_scores = vis_scores[keep_mask]\n",
    "        motion_vals = motion_vals[keep_mask]\n",
    "\n",
    "        # Resample to fixed FPS (lenient; keeps pauses)\n",
    "        frames = resample_frames(frames, orig_fps, TARGET_FPS)\n",
    "\n",
    "        # Light smoothing\n",
    "        frames = exponential_smooth(frames, alpha=EMA_ALPHA)\n",
    "\n",
    "        # Optional: trim/pad for downstream batching\n",
    "        frames_tp = trim_or_pad_sequence(frames, TARGET_SEQ_LEN)\n",
    "\n",
    "        motion_summary = summarize_motion(motion_vals)\n",
    "        mean_vis = float(vis_scores.mean()) if vis_scores.size else 0.0\n",
    "\n",
    "        status = 'success'\n",
    "        if (frames.shape[0] < MIN_FRAMES or detection_ratio < MIN_VALID_RATIO) and not KEEP_LOW_QUALITY:\n",
    "            status = 'low_quality'\n",
    "        elif (frames.shape[0] < MIN_FRAMES or detection_ratio < MIN_VALID_RATIO) and KEEP_LOW_QUALITY:\n",
    "            status = 'low_quality_kept'\n",
    "\n",
    "        base = os.path.splitext(video_file)[0]\n",
    "        npy_path = os.path.join(output_dir, f\"{base}.npy\")\n",
    "        np.save(npy_path, frames.astype(np.float32))\n",
    "\n",
    "        payload = {\n",
    "            'video_file': video_file,\n",
    "            'feature_path': npy_path,\n",
    "            'num_frames': int(frames.shape[0]),\n",
    "            'frame_feature_dim': int(frames.shape[1]),\n",
    "            'detection_ratio': detection_ratio,\n",
    "            'mean_visibility': mean_vis,\n",
    "            'motion_mean': float(motion_summary.get('mean', 0.0)),\n",
    "            'motion_p50': float(motion_summary.get('p50', 0.0)),\n",
    "            'motion_p95': float(motion_summary.get('p95', 0.0)),\n",
    "            'motion_max': float(motion_summary.get('max', 0.0)),\n",
    "            'sentence_id': row_dict.get('sentence_id'),\n",
    "            'sentence_text': row_dict.get('sentence'),\n",
    "            'sentence_gloss': row_dict.get('gloss', ''),\n",
    "            'orig_fps': float(orig_fps),\n",
    "            'status': status,\n",
    "        }\n",
    "        return payload\n",
    "\n",
    "    except Exception as exc:\n",
    "        return {'status': 'failed', 'video_file': video_file, 'error': str(exc)}\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            if cap is not None:\n",
    "                cap.release()\n",
    "            if local_holistic is not None:\n",
    "                local_holistic.close()\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb2fe4a",
   "metadata": {},
   "source": [
    "## Cell 7 ‚Äî Parallel Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ef1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = max(1, cpu_count() - 2)\n",
    "print(f\"üöÄ Starting extraction | workers={num_workers} | videos={len(df_meta)}\")\n",
    "\n",
    "video_records = df_meta.to_dict('records')\n",
    "process_func = partial(process_single_video, video_dir=VIDEO_DIR, output_dir=OUTPUT_DIR, face_downsample=FACE_DOWNSAMPLE)\n",
    "\n",
    "results = []\n",
    "with Pool(num_workers, initializer=worker_init, initargs=(FACE_DOWNSAMPLE,)) as pool:\n",
    "    for res in tqdm(pool.imap(process_func, video_records), total=len(video_records), desc=\"Processing\"):\n",
    "        results.append(res)\n",
    "\n",
    "processed_records = []\n",
    "failed = []\n",
    "missing = []\n",
    "lowq = []\n",
    "for r in results:\n",
    "    status = r.get('status')\n",
    "    if status == 'success' or status == 'low_quality_kept':\n",
    "        processed_records.append(r)\n",
    "        if status == 'low_quality_kept':\n",
    "            lowq.append({**r, 'kept': True})\n",
    "    elif status == 'low_quality':\n",
    "        lowq.append({**r, 'kept': False})\n",
    "    elif status == 'failed':\n",
    "        failed.append(r)\n",
    "    elif status == 'missing':\n",
    "        missing.append(r)\n",
    "\n",
    "print(f\"‚úÖ Processed: {len(processed_records)} | ‚ö†Ô∏è LowQ: {len(lowq)} | ‚ùå Failed: {len(failed)} | üö´ Missing: {len(missing)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb504d3",
   "metadata": {},
   "source": [
    "## Cell 8 ‚Äî Persist Metadata & Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d7d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = pd.DataFrame(processed_records)\n",
    "output_meta_path = \"proc/text2sign_processed_metadata.csv\"\n",
    "df_processed.to_csv(output_meta_path, index=False)\n",
    "print(f\"‚úÖ Saved processed metadata: {output_meta_path}\")\n",
    "\n",
    "if failed:\n",
    "    pd.DataFrame(failed).to_csv(\"proc/text2sign_failed_videos.csv\", index=False)\n",
    "if missing:\n",
    "    pd.DataFrame(missing).to_csv(\"proc/text2sign_missing_videos.csv\", index=False)\n",
    "if lowq:\n",
    "    pd.DataFrame(lowq).to_csv(\"proc/text2sign_low_quality.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda6ca08",
   "metadata": {},
   "source": [
    "## Cell 9 ‚Äî Global Stats (mean/std + lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9fce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_processed.empty:\n",
    "    total_frames = 0\n",
    "    sum_feats = np.zeros(PER_FRAME_FEATURES, dtype=np.float64)\n",
    "    sum_sq = np.zeros(PER_FRAME_FEATURES, dtype=np.float64)\n",
    "    seq_lengths = []\n",
    "    motion_means = []\n",
    "\n",
    "    for row in tqdm(df_processed.itertuples(), total=len(df_processed), desc=\"Stats\"):\n",
    "        arr = np.load(row.feature_path).astype(np.float32)\n",
    "        total_frames += arr.shape[0]\n",
    "        sum_feats += arr.sum(axis=0)\n",
    "        sum_sq += (arr ** 2).sum(axis=0)\n",
    "        seq_lengths.append(arr.shape[0])\n",
    "        if hasattr(row, 'motion_mean'):\n",
    "            motion_means.append(row.motion_mean)\n",
    "\n",
    "    feature_mean = sum_feats / max(total_frames, 1)\n",
    "    feature_var = sum_sq / max(total_frames, 1) - feature_mean ** 2\n",
    "    feature_std = np.sqrt(np.maximum(feature_var, 1e-8))\n",
    "\n",
    "    np.savez(\n",
    "        GLOBAL_STATS_PATH,\n",
    "        feature_mean=feature_mean.astype(np.float32),\n",
    "        feature_std=feature_std.astype(np.float32),\n",
    "        seq_lengths=np.array(seq_lengths, dtype=np.int32),\n",
    "        motion_means=np.array(motion_means, dtype=np.float32),\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Saved global stats ‚Üí {GLOBAL_STATS_PATH}\")\n",
    "    print(f\"   ‚Ä¢ Avg seq len: {np.mean(seq_lengths):.1f}\")\n",
    "    print(f\"   ‚Ä¢ 95p seq len: {np.percentile(seq_lengths, 95):.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b20c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "# Fail-safe: Kill script if it runs longer than 48 hours (in seconds)\n",
    "MAX_DURATION = 48 * 3600 \n",
    "start_time = time.time()\n",
    "\n",
    "# ... inside your processing loop ...\n",
    "if time.time() - start_time > MAX_DURATION:\n",
    "    print(\"‚ö†Ô∏è 48-hour safety limit reached. Shutting down to save credits.\")\n",
    "    sys.exit(0) # This will stop the execution, allowing the idle shutdown (if enabled) to kick in later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def5f195",
   "metadata": {},
   "source": [
    "## Cell 10 ‚Äî Quick Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb19e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"üìä TEXT2SIGN PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Videos processed: {len(df_processed)}\")\n",
    "print(f\"Features dir: {OUTPUT_DIR}\")\n",
    "print(f\"Metadata: proc/text2sign_processed_metadata.csv\")\n",
    "if os.path.exists(GLOBAL_STATS_PATH):\n",
    "    stats = np.load(GLOBAL_STATS_PATH)\n",
    "    print(f\"Mean/std saved with dim: {stats['feature_mean'].shape[0]}\")\n",
    "print(f\"Median frames: {df_processed['num_frames'].median() if not df_processed.empty else 0}\")\n",
    "print(f\"Median detection ratio: {df_processed['detection_ratio'].median() if 'dete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signtalk_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
